---
name: data-quality
description: æ•°æ®è´¨é‡ä¿éšœã€‚Great Expectationsã€dbtã€æ•°æ®éªŒè¯ã€æ•°æ®æµ‹è¯•ã€æ•°æ®è¡€ç¼˜ã€å®Œæ•´æ€§æ£€æŸ¥ã€‚å½“ç”¨æˆ·æåˆ°æ•°æ®è´¨é‡ã€Great Expectationsã€dbtã€æ•°æ®éªŒè¯ã€æ•°æ®æµ‹è¯•æ—¶ä½¿ç”¨ã€‚
---

# ğŸ¯ æ•°æ®è´¨é‡ç§˜å…¸ Â· Data Quality

## è´¨é‡ç»´åº¦

```
å®Œæ•´æ€§ â†’ å‡†ç¡®æ€§ â†’ ä¸€è‡´æ€§ â†’ åŠæ—¶æ€§ â†’ æœ‰æ•ˆæ€§
  â”‚        â”‚        â”‚        â”‚        â”‚
  â””â”€ éç©º â”€â”´â”€ èŒƒå›´ â”€â”´â”€ å…³è” â”€â”´â”€ æ–°é²œåº¦ â”€â”´â”€ æ ¼å¼
```

## Great Expectations åŸºç¡€

### å®‰è£…å’Œåˆå§‹åŒ–

```bash
# å®‰è£…
pip install great_expectations

# åˆå§‹åŒ–é¡¹ç›®
great_expectations init

# é¡¹ç›®ç»“æ„
great_expectations/
â”œâ”€â”€ great_expectations.yml
â”œâ”€â”€ expectations/
â”œâ”€â”€ checkpoints/
â”œâ”€â”€ plugins/
â””â”€â”€ uncommitted/
```

### åˆ›å»º Data Context

```python
import great_expectations as gx
from great_expectations.data_context import FileDataContext

# è·å– Data Context
context = gx.get_context()

# æ·»åŠ æ•°æ®æº
datasource = context.sources.add_pandas("my_datasource")

# æ·»åŠ æ•°æ®èµ„äº§
data_asset = datasource.add_dataframe_asset(name="users_df")

# æ„å»ºæ‰¹æ¬¡è¯·æ±‚
batch_request = data_asset.build_batch_request(dataframe=df)
```

### Expectations å®šä¹‰

```python
import pandas as pd
import great_expectations as gx

# åˆ›å»º Validator
context = gx.get_context()
validator = context.sources.pandas_default.read_dataframe(df)

# åŸºç¡€ Expectations
validator.expect_table_row_count_to_be_between(min_value=100, max_value=10000)
validator.expect_table_column_count_to_equal(value=5)

# åˆ—å­˜åœ¨æ€§
validator.expect_column_to_exist(column="user_id")
validator.expect_column_to_exist(column="email")

# éç©ºæ£€æŸ¥
validator.expect_column_values_to_not_be_null(column="user_id")
validator.expect_column_values_to_not_be_null(column="email")

# å”¯ä¸€æ€§æ£€æŸ¥
validator.expect_column_values_to_be_unique(column="user_id")
validator.expect_column_values_to_be_unique(column="email")

# å€¼èŒƒå›´æ£€æŸ¥
validator.expect_column_values_to_be_between(
    column="age",
    min_value=0,
    max_value=120
)

# å€¼é›†åˆæ£€æŸ¥
validator.expect_column_values_to_be_in_set(
    column="status",
    value_set=["active", "inactive", "pending"]
)

# æ­£åˆ™è¡¨è¾¾å¼æ£€æŸ¥
validator.expect_column_values_to_match_regex(
    column="email",
    regex=r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"
)

# ç±»å‹æ£€æŸ¥
validator.expect_column_values_to_be_of_type(
    column="age",
    type_="int64"
)

# ä¿å­˜ Expectation Suite
validator.save_expectation_suite(discard_failed_expectations=False)
```

### è‡ªå®šä¹‰ Expectations

```python
from great_expectations.expectations.expectation import ColumnMapExpectation
from great_expectations.execution_engine import PandasExecutionEngine

class ExpectColumnValuesToBeValidPhoneNumber(ColumnMapExpectation):
    """æœŸæœ›åˆ—å€¼ä¸ºæœ‰æ•ˆç”µè¯å·ç """

    map_metric = "column_values.match_phone_pattern"

    @classmethod
    def _atomic_prescriptive_template(cls, **kwargs):
        return "values must be valid phone numbers"

    @classmethod
    def _prescriptive_template(cls, **kwargs):
        return "At least $mostly_pct % of values in $column must be valid phone numbers"

# æ³¨å†Œè‡ªå®šä¹‰ Expectation
validator.expect_column_values_to_be_valid_phone_number(
    column="phone",
    mostly=0.95
)
```

### Checkpoints æ‰§è¡Œ

```python
# åˆ›å»º Checkpoint
checkpoint_config = {
    "name": "my_checkpoint",
    "config_version": 1.0,
    "class_name": "SimpleCheckpoint",
    "validations": [
        {
            "batch_request": {
                "datasource_name": "my_datasource",
                "data_asset_name": "users_df",
            },
            "expectation_suite_name": "users_suite",
        }
    ],
}

context.add_checkpoint(**checkpoint_config)

# è¿è¡Œ Checkpoint
result = context.run_checkpoint(
    checkpoint_name="my_checkpoint",
    batch_request=batch_request,
)

# æ£€æŸ¥ç»“æœ
if result["success"]:
    print("All expectations passed!")
else:
    print("Some expectations failed:")
    for validation in result["run_results"].values():
        for result in validation["validation_result"]["results"]:
            if not result["success"]:
                print(f"  - {result['expectation_config']['expectation_type']}")
```

### Data Docs ç”Ÿæˆ

```python
# æ„å»º Data Docs
context.build_data_docs()

# æ‰“å¼€ Data Docs
context.open_data_docs()

# è‡ªå®šä¹‰ Data Docs ç«™ç‚¹
data_docs_config = {
    "sites": {
        "local_site": {
            "class_name": "SiteBuilder",
            "store_backend": {
                "class_name": "TupleFilesystemStoreBackend",
                "base_directory": "uncommitted/data_docs/local_site/",
            },
            "site_index_builder": {
                "class_name": "DefaultSiteIndexBuilder",
            },
        }
    }
}
```

## dbt æ•°æ®æµ‹è¯•

### é¡¹ç›®ç»“æ„

```yaml
# dbt_project.yml
name: 'my_project'
version: '1.0.0'
config-version: 2

profile: 'default'

model-paths: ["models"]
test-paths: ["tests"]
seed-paths: ["seeds"]
macro-paths: ["macros"]

models:
  my_project:
    +materialized: table
```

### Schema æµ‹è¯•

```yaml
# models/schema.yml
version: 2

models:
  - name: users
    description: "User table"
    columns:
      - name: user_id
        description: "Primary key"
        tests:
          - unique
          - not_null

      - name: email
        description: "User email"
        tests:
          - unique
          - not_null
          - dbt_utils.email

      - name: age
        description: "User age"
        tests:
          - not_null
          - dbt_utils.accepted_range:
              min_value: 0
              max_value: 120

      - name: status
        description: "User status"
        tests:
          - not_null
          - accepted_values:
              values: ['active', 'inactive', 'pending']

      - name: created_at
        description: "Creation timestamp"
        tests:
          - not_null
          - dbt_utils.not_future_date

      - name: country_code
        description: "Country code"
        tests:
          - relationships:
              to: ref('countries')
              field: code
```

### è‡ªå®šä¹‰ Data æµ‹è¯•

```sql
-- tests/assert_positive_revenue.sql
-- æµ‹è¯•æ”¶å…¥å¿…é¡»ä¸ºæ­£æ•°

SELECT
    order_id,
    revenue
FROM {{ ref('orders') }}
WHERE revenue <= 0
```

```sql
-- tests/assert_user_email_domain.sql
-- æµ‹è¯•ç”¨æˆ·é‚®ç®±åŸŸå

SELECT
    user_id,
    email
FROM {{ ref('users') }}
WHERE email NOT LIKE '%@company.com'
  AND email NOT LIKE '%@partner.com'
```

### Generic æµ‹è¯•

```sql
-- macros/test_valid_date_range.sql
{% test valid_date_range(model, column_name, start_date, end_date) %}

SELECT *
FROM {{ model }}
WHERE {{ column_name }} < '{{ start_date }}'
   OR {{ column_name }} > '{{ end_date }}'

{% endtest %}
```

```yaml
# ä½¿ç”¨ Generic æµ‹è¯•
models:
  - name: events
    columns:
      - name: event_date
        tests:
          - valid_date_range:
              start_date: '2020-01-01'
              end_date: '2025-12-31'
```

### Singular æµ‹è¯•

```sql
-- tests/assert_revenue_consistency.sql
-- æµ‹è¯•æ”¶å…¥ä¸€è‡´æ€§

WITH order_revenue AS (
    SELECT SUM(amount) AS total
    FROM {{ ref('orders') }}
),
payment_revenue AS (
    SELECT SUM(amount) AS total
    FROM {{ ref('payments') }}
)

SELECT
    o.total AS order_total,
    p.total AS payment_total,
    ABS(o.total - p.total) AS difference
FROM order_revenue o
CROSS JOIN payment_revenue p
WHERE ABS(o.total - p.total) > 0.01
```

### dbt æµ‹è¯•æ‰§è¡Œ

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
dbt test

# è¿è¡Œç‰¹å®šæ¨¡å‹çš„æµ‹è¯•
dbt test --select users

# è¿è¡Œç‰¹å®šæµ‹è¯•
dbt test --select test_name:unique_users_user_id

# è¿è¡Œå¤±è´¥çš„æµ‹è¯•
dbt test --select result:fail

# å­˜å‚¨æµ‹è¯•å¤±è´¥è®°å½•
dbt test --store-failures
```

### dbt Expectations åŒ…

```yaml
# packages.yml
packages:
  - package: calogica/dbt_expectations
    version: 0.9.0
```

```yaml
# ä½¿ç”¨ dbt_expectations
models:
  - name: users
    columns:
      - name: email
        tests:
          - dbt_expectations.expect_column_values_to_match_regex:
              regex: "^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$"

      - name: age
        tests:
          - dbt_expectations.expect_column_mean_to_be_between:
              min_value: 18
              max_value: 65

      - name: created_at
        tests:
          - dbt_expectations.expect_row_values_to_have_recent_data:
              datepart: day
              interval: 7
```

## æ•°æ®éªŒè¯è§„åˆ™

### å®Œæ•´æ€§æ£€æŸ¥

```python
import pandas as pd

def check_completeness(df: pd.DataFrame, required_columns: list) -> dict:
    """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
    results = {}

    # æ£€æŸ¥å¿…éœ€åˆ—
    missing_columns = set(required_columns) - set(df.columns)
    results['missing_columns'] = list(missing_columns)

    # æ£€æŸ¥ç©ºå€¼
    null_counts = df[required_columns].isnull().sum()
    results['null_counts'] = null_counts.to_dict()

    # æ£€æŸ¥ç©ºå­—ç¬¦ä¸²
    for col in required_columns:
        if df[col].dtype == 'object':
            empty_count = (df[col] == '').sum()
            results[f'{col}_empty_count'] = empty_count

    return results

# ä½¿ç”¨ç¤ºä¾‹
required_cols = ['user_id', 'email', 'name']
completeness = check_completeness(df, required_cols)
```

### å‡†ç¡®æ€§æ£€æŸ¥

```python
def check_accuracy(df: pd.DataFrame) -> dict:
    """æ£€æŸ¥æ•°æ®å‡†ç¡®æ€§"""
    results = {}

    # æ•°å€¼èŒƒå›´æ£€æŸ¥
    if 'age' in df.columns:
        invalid_age = df[(df['age'] < 0) | (df['age'] > 120)]
        results['invalid_age_count'] = len(invalid_age)

    # æ ¼å¼æ£€æŸ¥
    if 'email' in df.columns:
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        invalid_email = df[~df['email'].str.match(email_pattern, na=False)]
        results['invalid_email_count'] = len(invalid_email)

    # é€»è¾‘æ£€æŸ¥
    if 'start_date' in df.columns and 'end_date' in df.columns:
        invalid_dates = df[df['start_date'] > df['end_date']]
        results['invalid_date_range_count'] = len(invalid_dates)

    return results
```

### ä¸€è‡´æ€§æ£€æŸ¥

```python
def check_consistency(df1: pd.DataFrame, df2: pd.DataFrame, key: str) -> dict:
    """æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§"""
    results = {}

    # ä¸»é”®ä¸€è‡´æ€§
    keys1 = set(df1[key])
    keys2 = set(df2[key])

    results['only_in_df1'] = len(keys1 - keys2)
    results['only_in_df2'] = len(keys2 - keys1)
    results['in_both'] = len(keys1 & keys2)

    # å€¼ä¸€è‡´æ€§
    merged = df1.merge(df2, on=key, suffixes=('_1', '_2'))
    for col in df1.columns:
        if col != key and f'{col}_2' in merged.columns:
            inconsistent = merged[merged[f'{col}_1'] != merged[f'{col}_2']]
            results[f'{col}_inconsistent_count'] = len(inconsistent)

    return results
```

### åŠæ—¶æ€§æ£€æŸ¥

```python
from datetime import datetime, timedelta

def check_timeliness(df: pd.DataFrame, timestamp_col: str, max_age_hours: int = 24) -> dict:
    """æ£€æŸ¥æ•°æ®åŠæ—¶æ€§"""
    results = {}

    df[timestamp_col] = pd.to_datetime(df[timestamp_col])
    now = datetime.now()
    threshold = now - timedelta(hours=max_age_hours)

    # è¿‡æœŸæ•°æ®
    stale_data = df[df[timestamp_col] < threshold]
    results['stale_count'] = len(stale_data)
    results['stale_percentage'] = len(stale_data) / len(df) * 100

    # æœ€æ–°æ•°æ®æ—¶é—´
    results['latest_timestamp'] = df[timestamp_col].max()
    results['oldest_timestamp'] = df[timestamp_col].min()
    results['data_age_hours'] = (now - df[timestamp_col].max()).total_seconds() / 3600

    return results
```

## æ•°æ®è¡€ç¼˜è¿½è¸ª

### dbt è¡€ç¼˜

```sql
-- models/staging/stg_users.sql
SELECT
    user_id,
    email,
    created_at
FROM {{ source('raw', 'users') }}

-- models/marts/dim_users.sql
SELECT
    user_id,
    email,
    DATE(created_at) AS created_date
FROM {{ ref('stg_users') }}

-- models/marts/fct_orders.sql
SELECT
    o.order_id,
    u.user_id,
    o.amount
FROM {{ ref('stg_orders') }} o
LEFT JOIN {{ ref('dim_users') }} u
    ON o.user_id = u.user_id
```

```bash
# ç”Ÿæˆè¡€ç¼˜å›¾
dbt docs generate
dbt docs serve

# æŸ¥çœ‹è¡€ç¼˜å…³ç³»
# http://localhost:8080
```

### è‡ªå®šä¹‰è¡€ç¼˜è¿½è¸ª

```python
from dataclasses import dataclass
from typing import List, Dict

@dataclass
class DataLineage:
    """æ•°æ®è¡€ç¼˜"""
    table_name: str
    upstream_tables: List[str]
    transformation: str
    created_at: str

class LineageTracker:
    """è¡€ç¼˜è¿½è¸ªå™¨"""

    def __init__(self):
        self.lineage: Dict[str, DataLineage] = {}

    def register(self, table_name: str, upstream: List[str], transformation: str):
        """æ³¨å†Œè¡€ç¼˜å…³ç³»"""
        self.lineage[table_name] = DataLineage(
            table_name=table_name,
            upstream_tables=upstream,
            transformation=transformation,
            created_at=datetime.now().isoformat()
        )

    def get_upstream(self, table_name: str, recursive: bool = False) -> List[str]:
        """è·å–ä¸Šæ¸¸è¡¨"""
        if table_name not in self.lineage:
            return []

        upstream = self.lineage[table_name].upstream_tables

        if recursive:
            all_upstream = set(upstream)
            for table in upstream:
                all_upstream.update(self.get_upstream(table, recursive=True))
            return list(all_upstream)

        return upstream

    def get_downstream(self, table_name: str) -> List[str]:
        """è·å–ä¸‹æ¸¸è¡¨"""
        downstream = []
        for name, lineage in self.lineage.items():
            if table_name in lineage.upstream_tables:
                downstream.append(name)
        return downstream

# ä½¿ç”¨ç¤ºä¾‹
tracker = LineageTracker()

tracker.register('stg_users', ['raw.users'], 'SELECT * FROM raw.users')
tracker.register('dim_users', ['stg_users'], 'SELECT user_id, email FROM stg_users')
tracker.register('fct_orders', ['stg_orders', 'dim_users'], 'JOIN transformation')

print(tracker.get_upstream('fct_orders', recursive=True))
# ['stg_orders', 'dim_users', 'stg_users', 'raw.users']
```

## æ•°æ®è´¨é‡ç›‘æ§

### è´¨é‡æŒ‡æ ‡è®¡ç®—

```python
import pandas as pd
from typing import Dict

class DataQualityMetrics:
    """æ•°æ®è´¨é‡æŒ‡æ ‡"""

    @staticmethod
    def calculate_completeness(df: pd.DataFrame) -> float:
        """å®Œæ•´æ€§å¾—åˆ†"""
        total_cells = df.size
        non_null_cells = df.count().sum()
        return (non_null_cells / total_cells) * 100

    @staticmethod
    def calculate_uniqueness(df: pd.DataFrame, key_columns: List[str]) -> float:
        """å”¯ä¸€æ€§å¾—åˆ†"""
        total_rows = len(df)
        unique_rows = df[key_columns].drop_duplicates().shape[0]
        return (unique_rows / total_rows) * 100

    @staticmethod
    def calculate_validity(df: pd.DataFrame, rules: Dict) -> float:
        """æœ‰æ•ˆæ€§å¾—åˆ†"""
        total_rows = len(df)
        valid_rows = total_rows

        for column, rule in rules.items():
            if rule['type'] == 'range':
                invalid = df[
                    (df[column] < rule['min']) | (df[column] > rule['max'])
                ]
                valid_rows -= len(invalid)
            elif rule['type'] == 'regex':
                invalid = df[~df[column].str.match(rule['pattern'], na=False)]
                valid_rows -= len(invalid)

        return (valid_rows / total_rows) * 100

    @staticmethod
    def calculate_overall_score(metrics: Dict[str, float]) -> float:
        """ç»¼åˆè´¨é‡å¾—åˆ†"""
        weights = {
            'completeness': 0.3,
            'uniqueness': 0.2,
            'validity': 0.3,
            'timeliness': 0.2,
        }

        score = sum(metrics.get(k, 0) * v for k, v in weights.items())
        return score

# ä½¿ç”¨ç¤ºä¾‹
metrics = DataQualityMetrics()

completeness = metrics.calculate_completeness(df)
uniqueness = metrics.calculate_uniqueness(df, ['user_id'])
validity = metrics.calculate_validity(df, {
    'age': {'type': 'range', 'min': 0, 'max': 120}
})

overall = metrics.calculate_overall_score({
    'completeness': completeness,
    'uniqueness': uniqueness,
    'validity': validity,
    'timeliness': 95.0,
})

print(f"Overall Quality Score: {overall:.2f}%")
```

### è´¨é‡å‘Šè­¦

```python
class QualityAlert:
    """è´¨é‡å‘Šè­¦"""

    def __init__(self, thresholds: Dict[str, float]):
        self.thresholds = thresholds

    def check_and_alert(self, metrics: Dict[str, float]) -> List[str]:
        """æ£€æŸ¥å¹¶ç”Ÿæˆå‘Šè­¦"""
        alerts = []

        for metric, value in metrics.items():
            threshold = self.thresholds.get(metric)
            if threshold and value < threshold:
                alerts.append(
                    f"ALERT: {metric} is {value:.2f}%, "
                    f"below threshold {threshold}%"
                )

        return alerts

# ä½¿ç”¨ç¤ºä¾‹
alert_system = QualityAlert({
    'completeness': 95.0,
    'uniqueness': 99.0,
    'validity': 98.0,
})

alerts = alert_system.check_and_alert({
    'completeness': 92.5,
    'uniqueness': 99.5,
    'validity': 97.0,
})

for alert in alerts:
    print(alert)
    # å‘é€é€šçŸ¥ï¼ˆSlack/Email/PagerDutyï¼‰
```

## Soda Core é›†æˆ

### å®‰è£…å’Œé…ç½®

```bash
# å®‰è£…
pip install soda-core-postgres

# é…ç½®
# configuration.yml
data_source my_datasource:
  type: postgres
  host: localhost
  port: 5432
  username: user
  password: pass
  database: mydb
```

### Checks å®šä¹‰

```yaml
# checks.yml
checks for users:
  - row_count > 100
  - missing_count(user_id) = 0
  - missing_count(email) = 0
  - duplicate_count(user_id) = 0
  - duplicate_count(email) = 0
  - invalid_count(email) = 0:
      valid format: email
  - invalid_count(age) = 0:
      valid min: 0
      valid max: 120
  - values in (status) must be in ['active', 'inactive', 'pending']
  - freshness(created_at) < 1d
```

### æ‰§è¡Œæ£€æŸ¥

```python
from soda.scan import Scan

# åˆ›å»ºæ‰«æ
scan = Scan()
scan.set_data_source_name("my_datasource")
scan.add_configuration_yaml_file("configuration.yml")
scan.add_sodacl_yaml_file("checks.yml")

# æ‰§è¡Œæ‰«æ
scan.execute()

# æ£€æŸ¥ç»“æœ
if scan.has_check_fails():
    print("Quality checks failed!")
    for check in scan.get_checks_fail():
        print(f"  - {check}")
else:
    print("All quality checks passed!")
```

## æœ€ä½³å®è·µ

### åˆ†å±‚éªŒè¯ç­–ç•¥

```python
# 1. æºæ•°æ®éªŒè¯
def validate_source(df: pd.DataFrame):
    """æºæ•°æ®éªŒè¯"""
    assert not df.empty, "Source data is empty"
    assert df['id'].is_unique, "Duplicate IDs in source"

# 2. è½¬æ¢éªŒè¯
def validate_transformation(input_df: pd.DataFrame, output_df: pd.DataFrame):
    """è½¬æ¢éªŒè¯"""
    assert len(output_df) <= len(input_df), "Row count increased"
    assert set(output_df['id']).issubset(set(input_df['id'])), "New IDs appeared"

# 3. ç›®æ ‡éªŒè¯
def validate_target(df: pd.DataFrame):
    """ç›®æ ‡éªŒè¯"""
    assert df['amount'].sum() > 0, "Total amount is zero"
    assert df['date'].max() >= pd.Timestamp.now() - pd.Timedelta(days=1), "Data is stale"
```

### æŒç»­è´¨é‡ç›‘æ§

```python
import schedule
import time

def run_quality_checks():
    """è¿è¡Œè´¨é‡æ£€æŸ¥"""
    df = load_data()

    metrics = {
        'completeness': calculate_completeness(df),
        'validity': calculate_validity(df),
        'timeliness': calculate_timeliness(df),
    }

    # è®°å½•æŒ‡æ ‡
    log_metrics(metrics)

    # æ£€æŸ¥å‘Šè­¦
    alerts = check_alerts(metrics)
    if alerts:
        send_notifications(alerts)

# å®šæ—¶æ‰§è¡Œ
schedule.every(1).hours.do(run_quality_checks)

while True:
    schedule.run_pending()
    time.sleep(60)
```

### è´¨é‡æŠ¥å‘Šç”Ÿæˆ

```python
def generate_quality_report(df: pd.DataFrame) -> str:
    """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
    report = []

    report.append("# Data Quality Report")
    report.append(f"Generated at: {datetime.now()}")
    report.append(f"\n## Dataset Overview")
    report.append(f"- Total Rows: {len(df)}")
    report.append(f"- Total Columns: {len(df.columns)}")

    report.append(f"\n## Completeness")
    null_counts = df.isnull().sum()
    for col, count in null_counts.items():
        if count > 0:
            pct = (count / len(df)) * 100
            report.append(f"- {col}: {count} nulls ({pct:.2f}%)")

    report.append(f"\n## Duplicates")
    duplicates = df.duplicated().sum()
    report.append(f"- Total Duplicates: {duplicates}")

    return "\n".join(report)
```

## å·¥å…·å¯¹æ¯”

| å·¥å…· | ä¼˜åŠ¿ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| Great Expectations | ä¸°å¯Œçš„ Expectationsã€Data Docs | Python ç”Ÿæ€ã€å¤æ‚éªŒè¯ |
| dbt | SQL åŸç”Ÿã€è¡€ç¼˜è¿½è¸ª | æ•°æ®ä»“åº“ã€è½¬æ¢æµ‹è¯• |
| Soda Core | ç®€æ´çš„ YAML é…ç½® | å¿«é€ŸéªŒè¯ã€CI/CD |
| Apache Griffin | å¤§æ•°æ®è´¨é‡ | Hadoop/Spark ç”Ÿæ€ |
| Deequ | Spark åŸç”Ÿ | å¤§è§„æ¨¡æ•°æ®éªŒè¯ |

## å·¥å…·æ¸…å•

| å·¥å…· | ç”¨é€” | æ¨èåœºæ™¯ |
|------|------|----------|
| Great Expectations | æ•°æ®éªŒè¯æ¡†æ¶ | Python æ•°æ®ç®¡é“ |
| dbt | æ•°æ®è½¬æ¢æµ‹è¯• | SQL æ•°æ®ä»“åº“ |
| Soda Core | æ•°æ®è´¨é‡æ£€æŸ¥ | è½»é‡çº§éªŒè¯ |
| Apache Griffin | å¤§æ•°æ®è´¨é‡ | Hadoop ç”Ÿæ€ |
| Deequ | Spark æ•°æ®è´¨é‡ | å¤§è§„æ¨¡æ•°æ® |
| Monte Carlo | æ•°æ®å¯è§‚æµ‹æ€§ | ä¼ä¸šçº§ç›‘æ§ |
| Datafold | æ•°æ® Diff | å˜æ›´éªŒè¯ |

## è§¦å‘è¯

æ•°æ®è´¨é‡ã€Great Expectationsã€dbtã€æ•°æ®éªŒè¯ã€æ•°æ®æµ‹è¯•ã€å®Œæ•´æ€§ã€å‡†ç¡®æ€§ã€ä¸€è‡´æ€§ã€æ•°æ®è¡€ç¼˜ã€è´¨é‡ç›‘æ§
