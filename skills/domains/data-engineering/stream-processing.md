---
name: stream-processing
description: æµå¼å¤„ç†ã€‚Kafka Streamsã€Flinkã€å®æ—¶å¤„ç†ã€æµå¼è®¡ç®—ã€çª—å£å‡½æ•°ã€çŠ¶æ€ç®¡ç†ã€‚å½“ç”¨æˆ·æåˆ°æµå¤„ç†ã€Kafka Streamsã€Flinkã€å®æ—¶å¤„ç†ã€æµå¼è®¡ç®—æ—¶ä½¿ç”¨ã€‚
---

# ğŸŒŠ æµå¤„ç†ç§˜å…¸ Â· Stream Processing

## æµå¤„ç†æ¶æ„

```
æ•°æ®æº â†’ æ‘„å– â†’ å¤„ç† â†’ èšåˆ â†’ è¾“å‡º
  â”‚       â”‚      â”‚      â”‚      â”‚
  â””â”€ Kafka â”€â”´â”€ è½¬æ¢ â”€â”´â”€ çª—å£ â”€â”´â”€ Sink
```

## Kafka Streams åŸºç¡€

### æ‹“æ‰‘æ„å»º

```java
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.kstream.*;
import java.util.Properties;

public class StreamProcessor {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "stream-processor");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG,
                  Serdes.String().getClass());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG,
                  Serdes.String().getClass());

        StreamsBuilder builder = new StreamsBuilder();

        // æ„å»ºæ‹“æ‰‘
        KStream<String, String> source = builder.stream("input-topic");

        KStream<String, String> processed = source
            .filter((key, value) -> value != null)
            .mapValues(value -> value.toUpperCase())
            .peek((key, value) ->
                System.out.println("Processed: " + key + " -> " + value));

        processed.to("output-topic");

        KafkaStreams streams = new KafkaStreams(builder.build(), props);
        streams.start();

        Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
    }
}
```

### æµè½¬æ¢æ“ä½œ

```java
// Map è½¬æ¢
KStream<String, Integer> lengths = stream
    .mapValues(value -> value.length());

// FlatMap å±•å¼€
KStream<String, String> words = stream
    .flatMapValues(value -> Arrays.asList(value.split("\\s+")));

// Filter è¿‡æ»¤
KStream<String, String> filtered = stream
    .filter((key, value) -> value.length() > 10);

// Branch åˆ†æ”¯
KStream<String, String>[] branches = stream.branch(
    (key, value) -> value.startsWith("A"),
    (key, value) -> value.startsWith("B"),
    (key, value) -> true  // é»˜è®¤åˆ†æ”¯
);

// Merge åˆå¹¶
KStream<String, String> merged = stream1.merge(stream2);
```

### çŠ¶æ€å­˜å‚¨

```java
import org.apache.kafka.streams.state.KeyValueStore;
import org.apache.kafka.streams.state.StoreBuilder;
import org.apache.kafka.streams.state.Stores;

// åˆ›å»ºçŠ¶æ€å­˜å‚¨
StoreBuilder<KeyValueStore<String, Long>> storeBuilder =
    Stores.keyValueStoreBuilder(
        Stores.persistentKeyValueStore("counts-store"),
        Serdes.String(),
        Serdes.Long()
    );

builder.addStateStore(storeBuilder);

// ä½¿ç”¨çŠ¶æ€å­˜å‚¨
stream.transform(() -> new Transformer<String, String, KeyValue<String, Long>>() {
    private KeyValueStore<String, Long> stateStore;

    @Override
    public void init(ProcessorContext context) {
        this.stateStore = context.getStateStore("counts-store");
    }

    @Override
    public KeyValue<String, Long> transform(String key, String value) {
        Long count = stateStore.get(key);
        if (count == null) count = 0L;
        count++;
        stateStore.put(key, count);
        return KeyValue.pair(key, count);
    }

    @Override
    public void close() {}
}, "counts-store");
```

### èšåˆæ“ä½œ

```java
// Count è®¡æ•°
KTable<String, Long> counts = stream
    .groupByKey()
    .count(Materialized.as("counts-store"));

// Aggregate èšåˆ
KTable<String, Double> averages = stream
    .groupByKey()
    .aggregate(
        () -> new AggregateValue(0.0, 0L),  // åˆå§‹åŒ–
        (key, value, aggregate) -> {
            aggregate.sum += Double.parseDouble(value);
            aggregate.count++;
            return aggregate;
        },
        Materialized.with(Serdes.String(), aggregateSerde)
    )
    .mapValues(agg -> agg.sum / agg.count);

// Reduce å½’çº¦
KTable<String, String> reduced = stream
    .groupByKey()
    .reduce((value1, value2) -> value1 + "," + value2);
```

### Join æ“ä½œ

```java
// Stream-Stream Join
KStream<String, String> joined = stream1.join(
    stream2,
    (value1, value2) -> value1 + "-" + value2,
    JoinWindows.ofTimeDifferenceWithNoGrace(Duration.ofMinutes(5)),
    StreamJoined.with(Serdes.String(), Serdes.String(), Serdes.String())
);

// Stream-Table Join
KStream<String, String> enriched = stream.join(
    table,
    (streamValue, tableValue) -> streamValue + "-" + tableValue
);

// Table-Table Join
KTable<String, String> tableJoined = table1.join(
    table2,
    (value1, value2) -> value1 + "-" + value2
);
```

## Flink DataStream API

### åŸºç¡€æµå¤„ç†

```java
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.api.common.functions.MapFunction;

public class FlinkStreamProcessor {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env =
            StreamExecutionEnvironment.getExecutionEnvironment();

        // ä» Kafka è¯»å–
        DataStream<String> stream = env
            .addSource(new FlinkKafkaConsumer<>(
                "input-topic",
                new SimpleStringSchema(),
                properties
            ));

        // è½¬æ¢å¤„ç†
        DataStream<String> processed = stream
            .filter(value -> value != null)
            .map(new MapFunction<String, String>() {
                @Override
                public String map(String value) {
                    return value.toUpperCase();
                }
            });

        // å†™å…¥ Kafka
        processed.addSink(new FlinkKafkaProducer<>(
            "output-topic",
            new SimpleStringSchema(),
            properties
        ));

        env.execute("Flink Stream Processor");
    }
}
```

### çª—å£å‡½æ•°

```java
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.api.windowing.windows.TimeWindow;

// æ»šåŠ¨çª—å£ (Tumbling Window)
DataStream<Tuple2<String, Long>> tumblingCounts = stream
    .keyBy(value -> value.getKey())
    .window(TumblingProcessingTimeWindows.of(Time.minutes(5)))
    .sum(1);

// æ»‘åŠ¨çª—å£ (Sliding Window)
DataStream<Tuple2<String, Long>> slidingCounts = stream
    .keyBy(value -> value.getKey())
    .window(SlidingProcessingTimeWindows.of(
        Time.minutes(10),  // çª—å£å¤§å°
        Time.minutes(5)    // æ»‘åŠ¨æ­¥é•¿
    ))
    .sum(1);

// ä¼šè¯çª—å£ (Session Window)
DataStream<Tuple2<String, Long>> sessionCounts = stream
    .keyBy(value -> value.getKey())
    .window(ProcessingTimeSessionWindows.withGap(Time.minutes(10)))
    .sum(1);

// å…¨å±€çª—å£ (Global Window)
DataStream<Tuple2<String, Long>> globalCounts = stream
    .keyBy(value -> value.getKey())
    .window(GlobalWindows.create())
    .trigger(CountTrigger.of(100))  // æ¯100æ¡è§¦å‘
    .sum(1);
```

### çª—å£èšåˆ

```java
import org.apache.flink.streaming.api.functions.windowing.WindowFunction;
import org.apache.flink.util.Collector;

// å¢é‡èšåˆ + å…¨çª—å£å‡½æ•°
DataStream<String> result = stream
    .keyBy(value -> value.getKey())
    .window(TumblingProcessingTimeWindows.of(Time.minutes(5)))
    .aggregate(
        new AverageAggregate(),  // å¢é‡èšåˆ
        new WindowResultFunction()  // å…¨çª—å£å¤„ç†
    );

// AverageAggregate å®ç°
class AverageAggregate implements AggregateFunction<
    Tuple2<String, Double>,
    Tuple2<Double, Long>,
    Double> {

    @Override
    public Tuple2<Double, Long> createAccumulator() {
        return new Tuple2<>(0.0, 0L);
    }

    @Override
    public Tuple2<Double, Long> add(
        Tuple2<String, Double> value,
        Tuple2<Double, Long> accumulator) {
        return new Tuple2<>(
            accumulator.f0 + value.f1,
            accumulator.f1 + 1L
        );
    }

    @Override
    public Double getResult(Tuple2<Double, Long> accumulator) {
        return accumulator.f0 / accumulator.f1;
    }

    @Override
    public Tuple2<Double, Long> merge(
        Tuple2<Double, Long> a,
        Tuple2<Double, Long> b) {
        return new Tuple2<>(a.f0 + b.f0, a.f1 + b.f1);
    }
}
```

### ProcessFunction

```java
import org.apache.flink.streaming.api.functions.ProcessFunction;
import org.apache.flink.util.Collector;

// ä½çº§ API - å®Œå…¨æ§åˆ¶
DataStream<String> processed = stream.process(
    new ProcessFunction<String, String>() {
        @Override
        public void processElement(
            String value,
            Context ctx,
            Collector<String> out) throws Exception {

            // è®¿é—®æ—¶é—´æˆ³
            long timestamp = ctx.timestamp();

            // æ³¨å†Œå®šæ—¶å™¨
            ctx.timerService().registerProcessingTimeTimer(
                timestamp + 60000
            );

            // è¾“å‡ºç»“æœ
            out.collect(value.toUpperCase());
        }

        @Override
        public void onTimer(
            long timestamp,
            OnTimerContext ctx,
            Collector<String> out) throws Exception {
            // å®šæ—¶å™¨è§¦å‘
            out.collect("Timer fired at " + timestamp);
        }
    }
);
```

### çŠ¶æ€ç®¡ç†

```java
import org.apache.flink.api.common.state.*;
import org.apache.flink.configuration.Configuration;

// ValueState - å•å€¼çŠ¶æ€
class StatefulMapFunction extends RichMapFunction<String, String> {
    private transient ValueState<Long> countState;

    @Override
    public void open(Configuration parameters) {
        ValueStateDescriptor<Long> descriptor =
            new ValueStateDescriptor<>("count", Long.class, 0L);
        countState = getRuntimeContext().getState(descriptor);
    }

    @Override
    public String map(String value) throws Exception {
        Long count = countState.value();
        count++;
        countState.update(count);
        return value + " (count: " + count + ")";
    }
}

// ListState - åˆ—è¡¨çŠ¶æ€
class ListStateFunction extends RichFlatMapFunction<String, String> {
    private transient ListState<String> listState;

    @Override
    public void open(Configuration parameters) {
        ListStateDescriptor<String> descriptor =
            new ListStateDescriptor<>("list", String.class);
        listState = getRuntimeContext().getListState(descriptor);
    }

    @Override
    public void flatMap(String value, Collector<String> out) throws Exception {
        listState.add(value);

        // è¾“å‡ºæ‰€æœ‰å†å²å€¼
        for (String item : listState.get()) {
            out.collect(item);
        }
    }
}

// MapState - æ˜ å°„çŠ¶æ€
class MapStateFunction extends RichFlatMapFunction<
    Tuple2<String, String>, String> {

    private transient MapState<String, Long> mapState;

    @Override
    public void open(Configuration parameters) {
        MapStateDescriptor<String, Long> descriptor =
            new MapStateDescriptor<>("map", String.class, Long.class);
        mapState = getRuntimeContext().getMapState(descriptor);
    }

    @Override
    public void flatMap(
        Tuple2<String, String> value,
        Collector<String> out) throws Exception {

        String key = value.f1;
        Long count = mapState.get(key);
        if (count == null) count = 0L;
        count++;
        mapState.put(key, count);

        out.collect(key + ": " + count);
    }
}
```

### Checkpoint å’Œ Savepoint

```java
// å¯ç”¨ Checkpoint
env.enableCheckpointing(60000);  // æ¯60ç§’

// Checkpoint é…ç½®
env.getCheckpointConfig().setCheckpointingMode(
    CheckpointingMode.EXACTLY_ONCE
);
env.getCheckpointConfig().setMinPauseBetweenCheckpoints(30000);
env.getCheckpointConfig().setCheckpointTimeout(600000);
env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);

// å¤–éƒ¨åŒ– Checkpoint
env.getCheckpointConfig().enableExternalizedCheckpoints(
    ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION
);

// ä» Savepoint æ¢å¤
// flink run -s /path/to/savepoint your-job.jar
```

## çª—å£ç±»å‹å¯¹æ¯”

| çª—å£ç±»å‹ | ç‰¹ç‚¹ | ä½¿ç”¨åœºæ™¯ |
|----------|------|----------|
| æ»šåŠ¨çª—å£ | å›ºå®šå¤§å°ï¼Œæ— é‡å  | æ¯å°æ—¶ç»Ÿè®¡ã€æ—¥æŠ¥ |
| æ»‘åŠ¨çª—å£ | å›ºå®šå¤§å°ï¼Œæœ‰é‡å  | ç§»åŠ¨å¹³å‡ã€è¶‹åŠ¿åˆ†æ |
| ä¼šè¯çª—å£ | åŠ¨æ€å¤§å°ï¼ŒåŸºäºé—´éš” | ç”¨æˆ·ä¼šè¯ã€æ´»åŠ¨æ£€æµ‹ |
| å…¨å±€çª—å£ | æ— æ—¶é—´é™åˆ¶ | è‡ªå®šä¹‰è§¦å‘é€»è¾‘ |

## æ—¶é—´è¯­ä¹‰

### Event Time vs Processing Time

```java
// Event Time - äº‹ä»¶æ—¶é—´
env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);

DataStream<Event> stream = env
    .addSource(new EventSource())
    .assignTimestampsAndWatermarks(
        WatermarkStrategy
            .<Event>forBoundedOutOfOrderness(Duration.ofSeconds(10))
            .withTimestampAssigner((event, timestamp) -> event.getTimestamp())
    );

// Processing Time - å¤„ç†æ—¶é—´
env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);

DataStream<Event> stream = env
    .addSource(new EventSource())
    .window(TumblingProcessingTimeWindows.of(Time.minutes(5)));
```

### Watermark ç”Ÿæˆ

```java
// å‘¨æœŸæ€§ Watermark
class PeriodicWatermarkGenerator implements WatermarkGenerator<Event> {
    private long maxTimestamp = Long.MIN_VALUE;
    private final long maxOutOfOrderness = 5000;

    @Override
    public void onEvent(Event event, long eventTimestamp, WatermarkOutput output) {
        maxTimestamp = Math.max(maxTimestamp, eventTimestamp);
    }

    @Override
    public void onPeriodicEmit(WatermarkOutput output) {
        output.emitWatermark(new Watermark(maxTimestamp - maxOutOfOrderness));
    }
}

// æ ‡ç‚¹ Watermark
class PunctuatedWatermarkGenerator implements WatermarkGenerator<Event> {
    @Override
    public void onEvent(Event event, long eventTimestamp, WatermarkOutput output) {
        if (event.hasWatermarkMarker()) {
            output.emitWatermark(new Watermark(eventTimestamp));
        }
    }

    @Override
    public void onPeriodicEmit(WatermarkOutput output) {
        // ä¸éœ€è¦å‘¨æœŸæ€§å‘å°„
    }
}
```

## èƒŒå‹å¤„ç†

### Kafka Streams èƒŒå‹

```java
// é…ç½®æ¶ˆè´¹è€…
props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 500);
props.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG, 1024);
props.put(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG, 500);

// é…ç½®ç”Ÿäº§è€…
props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432);
props.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);
props.put(ProducerConfig.LINGER_MS_CONFIG, 10);
```

### Flink èƒŒå‹ç›‘æ§

```java
// é…ç½®ç¼“å†²åŒº
env.setBufferTimeout(100);

// ç›‘æ§èƒŒå‹
// Web UI -> Job -> BackPressure

// è°ƒæ•´å¹¶è¡Œåº¦
stream.map(new MyMapFunction()).setParallelism(4);
```

## å®¹é”™æœºåˆ¶

### Exactly-Once è¯­ä¹‰

```java
// Kafka Streams Exactly-Once
props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG,
          StreamsConfig.EXACTLY_ONCE_V2);

// Flink Exactly-Once
env.enableCheckpointing(60000);
env.getCheckpointConfig().setCheckpointingMode(
    CheckpointingMode.EXACTLY_ONCE
);

// Kafka Sink Exactly-Once
FlinkKafkaProducer<String> producer = new FlinkKafkaProducer<>(
    "output-topic",
    new SimpleStringSchema(),
    properties,
    FlinkKafkaProducer.Semantic.EXACTLY_ONCE
);
```

### æ•…éšœæ¢å¤

```java
// Kafka Streams è‡ªåŠ¨æ¢å¤
// çŠ¶æ€å­˜å‚¨è‡ªåŠ¨ä» Changelog Topic æ¢å¤

// Flink ä» Checkpoint æ¢å¤
// è‡ªåŠ¨ä»æœ€è¿‘çš„ Checkpoint æ¢å¤

// æ‰‹åŠ¨ä» Savepoint æ¢å¤
// flink run -s /path/to/savepoint your-job.jar
```

## æ€§èƒ½ä¼˜åŒ–

### Kafka Streams ä¼˜åŒ–

```java
// å¢åŠ å¹¶è¡Œåº¦
props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 4);

// ä¼˜åŒ–çŠ¶æ€å­˜å‚¨
props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 10 * 1024 * 1024);
props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);

// RocksDB é…ç½®
props.put(StreamsConfig.ROCKSDB_CONFIG_SETTER_CLASS_CONFIG,
          CustomRocksDBConfig.class);
```

### Flink ä¼˜åŒ–

```java
// è°ƒæ•´å¹¶è¡Œåº¦
env.setParallelism(8);

// é…ç½®å†…å­˜
env.getConfig().setTaskManagerMemory(MemorySize.ofMebiBytes(2048));

// å¯ç”¨å¯¹è±¡é‡ç”¨
env.getConfig().enableObjectReuse();

// é…ç½®ç½‘ç»œç¼“å†²åŒº
env.getConfig().setNetworkBufferMemory(64 * 1024 * 1024);
```

## Python API

### Kafka Streams Python (kafka-python)

```python
from kafka import KafkaConsumer, KafkaProducer
import json

consumer = KafkaConsumer(
    'input-topic',
    bootstrap_servers=['localhost:9092'],
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

for message in consumer:
    value = message.value

    # å¤„ç†é€»è¾‘
    processed = {
        'key': value['key'],
        'value': value['value'].upper()
    }

    producer.send('output-topic', processed)
    producer.flush()
```

### PyFlink

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors import FlinkKafkaConsumer, FlinkKafkaProducer
from pyflink.common.serialization import SimpleStringSchema

env = StreamExecutionEnvironment.get_execution_environment()

# ä» Kafka è¯»å–
kafka_consumer = FlinkKafkaConsumer(
    topics='input-topic',
    deserialization_schema=SimpleStringSchema(),
    properties={'bootstrap.servers': 'localhost:9092'}
)

stream = env.add_source(kafka_consumer)

# è½¬æ¢å¤„ç†
processed = stream \
    .filter(lambda x: x is not None) \
    .map(lambda x: x.upper())

# å†™å…¥ Kafka
kafka_producer = FlinkKafkaProducer(
    topic='output-topic',
    serialization_schema=SimpleStringSchema(),
    producer_config={'bootstrap.servers': 'localhost:9092'}
)

processed.add_sink(kafka_producer)

env.execute("PyFlink Stream Processor")
```

## ç›‘æ§æŒ‡æ ‡

### Kafka Streams æŒ‡æ ‡

```java
// JMX æŒ‡æ ‡
// kafka.streams:type=stream-metrics,client-id=*
// - commit-latency-avg
// - poll-latency-avg
// - process-latency-avg

// è‡ªå®šä¹‰æŒ‡æ ‡
StreamsMetrics metrics = context.metrics();
Sensor sensor = metrics.addLatencySensor(
    "process",
    "latency",
    "Process latency"
);
```

### Flink æŒ‡æ ‡

```java
// æ³¨å†ŒæŒ‡æ ‡
public class MyMapFunction extends RichMapFunction<String, String> {
    private transient Counter counter;

    @Override
    public void open(Configuration parameters) {
        this.counter = getRuntimeContext()
            .getMetricGroup()
            .counter("myCounter");
    }

    @Override
    public String map(String value) {
        counter.inc();
        return value.toUpperCase();
    }
}
```

## æœ€ä½³å®è·µ

### çŠ¶æ€å¤§å°æ§åˆ¶

```java
// ä½¿ç”¨ TTL æ¸…ç†è¿‡æœŸçŠ¶æ€
StateTtlConfig ttlConfig = StateTtlConfig
    .newBuilder(Time.hours(24))
    .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)
    .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)
    .build();

ValueStateDescriptor<Long> descriptor =
    new ValueStateDescriptor<>("count", Long.class);
descriptor.enableTimeToLive(ttlConfig);
```

### æ•°æ®å€¾æ–œå¤„ç†

```java
// æ·»åŠ éšæœºå‰ç¼€
stream
    .map(value -> {
        String randomPrefix = String.valueOf(new Random().nextInt(10));
        return Tuple2.of(randomPrefix + "-" + value.getKey(), value);
    })
    .keyBy(tuple -> tuple.f0)
    .window(TumblingProcessingTimeWindows.of(Time.minutes(5)))
    .sum(1);
```

### å»¶è¿Ÿæ•°æ®å¤„ç†

```java
// ä½¿ç”¨ä¾§è¾“å‡ºæµå¤„ç†å»¶è¿Ÿæ•°æ®
OutputTag<Event> lateOutputTag = new OutputTag<Event>("late-data"){};

DataStream<Event> result = stream
    .keyBy(Event::getKey)
    .window(TumblingEventTimeWindows.of(Time.minutes(5)))
    .allowedLateness(Time.minutes(1))
    .sideOutputLateData(lateOutputTag)
    .sum("value");

DataStream<Event> lateStream = result.getSideOutput(lateOutputTag);
```

## æ¡†æ¶å¯¹æ¯”

| ç‰¹æ€§ | Kafka Streams | Flink | Spark Streaming |
|------|---------------|-------|-----------------|
| éƒ¨ç½²æ¨¡å¼ | åµŒå…¥å¼ | ç‹¬ç«‹é›†ç¾¤ | ç‹¬ç«‹é›†ç¾¤ |
| çŠ¶æ€ç®¡ç† | RocksDB | å†…å­˜/RocksDB | å†…å­˜ |
| Exactly-Once | âœ… | âœ… | âœ… |
| çª—å£ç±»å‹ | ä¸°å¯Œ | æœ€ä¸°å¯Œ | åŸºç¡€ |
| å­¦ä¹ æ›²çº¿ | å¹³ç¼“ | é™¡å³­ | ä¸­ç­‰ |
| ç”Ÿæ€é›†æˆ | Kafka ç”Ÿæ€ | å¹¿æ³› | Spark ç”Ÿæ€ |

## å·¥å…·æ¸…å•

| å·¥å…· | ç”¨é€” | æ¨èåœºæ™¯ |
|------|------|----------|
| Kafka Streams | è½»é‡çº§æµå¤„ç† | Kafka ç”Ÿæ€ã€ç®€å•è½¬æ¢ |
| Apache Flink | åˆ†å¸ƒå¼æµå¤„ç† | å¤æ‚çª—å£ã€çŠ¶æ€ç®¡ç† |
| Spark Streaming | æ‰¹æµä¸€ä½“ | Spark ç”Ÿæ€ã€æ‰¹æµæ··åˆ |
| Apache Storm | å®æ—¶è®¡ç®— | ä½å»¶è¿Ÿã€ç®€å•æ‹“æ‰‘ |
| Apache Samza | LinkedIn æµå¤„ç† | Kafka + YARN |
| Pulsar Functions | Pulsar æµå¤„ç† | Pulsar ç”Ÿæ€ |

## è§¦å‘è¯

æµå¤„ç†ã€Kafka Streamsã€Flinkã€å®æ—¶å¤„ç†ã€æµå¼è®¡ç®—ã€çª—å£å‡½æ•°ã€çŠ¶æ€ç®¡ç†ã€Checkpointã€Watermarkã€èƒŒå‹
