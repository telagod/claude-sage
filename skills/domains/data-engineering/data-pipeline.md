---
name: data-pipeline
description: æ•°æ®ç®¡é“ç¼–æ’ã€‚Airflowã€Dagsterã€Prefectã€ETLã€æ•°æ®ç¼–æ’ã€è°ƒåº¦ç­–ç•¥ã€‚å½“ç”¨æˆ·æåˆ°æ•°æ®ç®¡é“ã€Airflowã€Dagsterã€Prefectã€ETLã€æ•°æ®ç¼–æ’æ—¶ä½¿ç”¨ã€‚
---

# ğŸ”„ æ•°æ®ç®¡é“ç§˜å…¸ Â· Data Pipeline

## ç®¡é“æ¶æ„

```
æ•°æ®æº â†’ æå– â†’ è½¬æ¢ â†’ åŠ è½½ â†’ ç›®æ ‡
  â”‚       â”‚      â”‚      â”‚      â”‚
  â””â”€ API â”€â”´â”€ æ¸…æ´— â”€â”´â”€ èšåˆ â”€â”´â”€ å­˜å‚¨
```

## Airflow DAG å¼€å‘

### åŸºç¡€ DAG ç»“æ„

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'etl_pipeline',
    default_args=default_args,
    description='ETL pipeline for user data',
    schedule_interval='0 2 * * *',  # æ¯å¤©å‡Œæ™¨2ç‚¹
    catchup=False,
    tags=['etl', 'production'],
) as dag:

    def extract_data(**context):
        """æå–æ•°æ®"""
        execution_date = context['execution_date']
        # æå–é€»è¾‘
        return {'records': 1000}

    def transform_data(**context):
        """è½¬æ¢æ•°æ®"""
        ti = context['ti']
        data = ti.xcom_pull(task_ids='extract')
        # è½¬æ¢é€»è¾‘
        return {'processed': data['records']}

    def load_data(**context):
        """åŠ è½½æ•°æ®"""
        ti = context['ti']
        data = ti.xcom_pull(task_ids='transform')
        # åŠ è½½é€»è¾‘
        print(f"Loaded {data['processed']} records")

    extract = PythonOperator(
        task_id='extract',
        python_callable=extract_data,
    )

    transform = PythonOperator(
        task_id='transform',
        python_callable=transform_data,
    )

    load = PythonOperator(
        task_id='load',
        python_callable=load_data,
    )

    extract >> transform >> load
```

### Operators ä½¿ç”¨

```python
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.http.operators.http import SimpleHttpOperator
from airflow.providers.amazon.aws.operators.s3 import S3CreateBucketOperator

# SQL Operator
create_table = PostgresOperator(
    task_id='create_table',
    postgres_conn_id='postgres_default',
    sql="""
        CREATE TABLE IF NOT EXISTS user_stats (
            date DATE,
            user_count INT,
            active_count INT
        );
    """,
)

# HTTP Operator
fetch_api = SimpleHttpOperator(
    task_id='fetch_api',
    http_conn_id='api_default',
    endpoint='/users',
    method='GET',
    headers={'Authorization': 'Bearer {{ var.value.api_token }}'},
    response_filter=lambda response: response.json(),
)

# S3 Operator
upload_to_s3 = S3CreateBucketOperator(
    task_id='upload_to_s3',
    bucket_name='data-lake-{{ ds_nodash }}',
    aws_conn_id='aws_default',
)
```

### Sensors ä½¿ç”¨

```python
from airflow.sensors.filesystem import FileSensor
from airflow.providers.http.sensors.http import HttpSensor
from airflow.sensors.external_task import ExternalTaskSensor

# æ–‡ä»¶ä¼ æ„Ÿå™¨
wait_for_file = FileSensor(
    task_id='wait_for_file',
    filepath='/data/input/{{ ds }}/data.csv',
    poke_interval=60,  # æ¯60ç§’æ£€æŸ¥ä¸€æ¬¡
    timeout=3600,  # 1å°æ—¶è¶…æ—¶
    mode='poke',
)

# HTTP ä¼ æ„Ÿå™¨
wait_for_api = HttpSensor(
    task_id='wait_for_api',
    http_conn_id='api_default',
    endpoint='/health',
    request_params={},
    response_check=lambda response: response.status_code == 200,
    poke_interval=30,
)

# å¤–éƒ¨ä»»åŠ¡ä¼ æ„Ÿå™¨
wait_for_upstream = ExternalTaskSensor(
    task_id='wait_for_upstream',
    external_dag_id='upstream_dag',
    external_task_id='final_task',
    execution_delta=timedelta(hours=1),
)
```

### XCom æ•°æ®ä¼ é€’

```python
from airflow.decorators import task

@task
def extract_data():
    """ä½¿ç”¨ TaskFlow API"""
    data = {'users': [1, 2, 3], 'count': 3}
    return data

@task
def transform_data(data: dict):
    """è‡ªåŠ¨æ¥æ”¶ä¸Šæ¸¸æ•°æ®"""
    transformed = {
        'users': [u * 2 for u in data['users']],
        'count': data['count']
    }
    return transformed

@task
def load_data(data: dict):
    """åŠ è½½æ•°æ®"""
    print(f"Loading {data['count']} users")

# é“¾å¼è°ƒç”¨
data = extract_data()
transformed = transform_data(data)
load_data(transformed)
```

### åŠ¨æ€ä»»åŠ¡ç”Ÿæˆ

```python
from airflow.decorators import task

@task
def get_partitions():
    """è·å–åˆ†åŒºåˆ—è¡¨"""
    return ['2024-01', '2024-02', '2024-03']

@task
def process_partition(partition: str):
    """å¤„ç†å•ä¸ªåˆ†åŒº"""
    print(f"Processing {partition}")

# åŠ¨æ€ç”Ÿæˆä»»åŠ¡
partitions = get_partitions()
process_partition.expand(partition=partitions)
```

## Dagster èµ„æºç®¡ç†

### Assets å®šä¹‰

```python
from dagster import asset, AssetExecutionContext, MaterializeResult
import pandas as pd

@asset(
    description="Raw user data from API",
    group_name="ingestion",
    compute_kind="python",
)
def raw_users(context: AssetExecutionContext) -> pd.DataFrame:
    """æå–åŸå§‹ç”¨æˆ·æ•°æ®"""
    context.log.info("Fetching users from API")
    df = pd.DataFrame({
        'user_id': [1, 2, 3],
        'name': ['Alice', 'Bob', 'Charlie']
    })
    return df

@asset(
    description="Cleaned user data",
    group_name="transformation",
    deps=[raw_users],
)
def cleaned_users(context: AssetExecutionContext, raw_users: pd.DataFrame) -> pd.DataFrame:
    """æ¸…æ´—ç”¨æˆ·æ•°æ®"""
    context.log.info(f"Cleaning {len(raw_users)} users")
    df = raw_users.dropna()
    df['name'] = df['name'].str.upper()
    return df

@asset(
    description="User statistics",
    group_name="analytics",
    deps=[cleaned_users],
)
def user_stats(context: AssetExecutionContext, cleaned_users: pd.DataFrame) -> MaterializeResult:
    """è®¡ç®—ç”¨æˆ·ç»Ÿè®¡"""
    count = len(cleaned_users)
    context.log.info(f"Total users: {count}")

    return MaterializeResult(
        metadata={
            "user_count": count,
            "preview": cleaned_users.head().to_markdown(),
        }
    )
```

### Resources é…ç½®

```python
from dagster import resource, ConfigurableResource
from pydantic import Field
import psycopg2

class PostgresResource(ConfigurableResource):
    """Postgres èµ„æº"""
    host: str = Field(description="Database host")
    port: int = Field(default=5432)
    database: str
    user: str
    password: str

    def get_connection(self):
        return psycopg2.connect(
            host=self.host,
            port=self.port,
            database=self.database,
            user=self.user,
            password=self.password,
        )

@asset
def users_from_db(postgres: PostgresResource) -> pd.DataFrame:
    """ä»æ•°æ®åº“è¯»å–ç”¨æˆ·"""
    conn = postgres.get_connection()
    df = pd.read_sql("SELECT * FROM users", conn)
    conn.close()
    return df
```

### Jobs å’Œ Schedules

```python
from dagster import define_asset_job, ScheduleDefinition, AssetSelection

# å®šä¹‰ Job
etl_job = define_asset_job(
    name="etl_job",
    selection=AssetSelection.groups("ingestion", "transformation"),
    description="ETL pipeline job",
)

analytics_job = define_asset_job(
    name="analytics_job",
    selection=AssetSelection.groups("analytics"),
)

# å®šä¹‰ Schedule
daily_schedule = ScheduleDefinition(
    job=etl_job,
    cron_schedule="0 2 * * *",  # æ¯å¤©å‡Œæ™¨2ç‚¹
)

hourly_schedule = ScheduleDefinition(
    job=analytics_job,
    cron_schedule="0 * * * *",  # æ¯å°æ—¶
)
```

### Sensors ç›‘å¬

```python
from dagster import sensor, RunRequest, SensorEvaluationContext
import os

@sensor(
    job=etl_job,
    minimum_interval_seconds=60,
)
def file_sensor(context: SensorEvaluationContext):
    """ç›‘å¬æ–‡ä»¶åˆ°è¾¾"""
    files = os.listdir('/data/input')
    for file in files:
        if file.endswith('.csv'):
            yield RunRequest(
                run_key=file,
                run_config={
                    "ops": {
                        "process_file": {
                            "config": {"filename": file}
                        }
                    }
                }
            )
```

### Partitions åˆ†åŒº

```python
from dagster import DailyPartitionsDefinition, asset

daily_partitions = DailyPartitionsDefinition(start_date="2024-01-01")

@asset(
    partitions_def=daily_partitions,
)
def daily_users(context: AssetExecutionContext) -> pd.DataFrame:
    """æŒ‰æ—¥åˆ†åŒºçš„ç”¨æˆ·æ•°æ®"""
    partition_date = context.partition_key
    context.log.info(f"Processing partition: {partition_date}")
    # å¤„ç†ç‰¹å®šæ—¥æœŸçš„æ•°æ®
    return pd.DataFrame()
```

## Prefect å·¥ä½œæµ

### Tasks å’Œ Flows

```python
from prefect import task, flow
from prefect.tasks import task_input_hash
from datetime import timedelta

@task(
    retries=3,
    retry_delay_seconds=60,
    cache_key_fn=task_input_hash,
    cache_expiration=timedelta(hours=1),
)
def extract_data(source: str) -> dict:
    """æå–æ•°æ®ä»»åŠ¡"""
    print(f"Extracting from {source}")
    return {'records': 1000}

@task
def transform_data(data: dict) -> dict:
    """è½¬æ¢æ•°æ®ä»»åŠ¡"""
    print(f"Transforming {data['records']} records")
    return {'processed': data['records']}

@task
def load_data(data: dict):
    """åŠ è½½æ•°æ®ä»»åŠ¡"""
    print(f"Loading {data['processed']} records")

@flow(name="ETL Pipeline", log_prints=True)
def etl_flow(source: str = "api"):
    """ETL å·¥ä½œæµ"""
    raw_data = extract_data(source)
    transformed = transform_data(raw_data)
    load_data(transformed)
```

### å¹¶å‘æ§åˆ¶

```python
from prefect import flow, task
from prefect.task_runners import ConcurrentTaskRunner

@task
def process_item(item: int) -> int:
    """å¤„ç†å•ä¸ªé¡¹ç›®"""
    return item * 2

@flow(task_runner=ConcurrentTaskRunner())
def parallel_flow():
    """å¹¶å‘æ‰§è¡Œä»»åŠ¡"""
    items = range(10)
    results = process_item.map(items)
    return results
```

### Deployments éƒ¨ç½²

```python
from prefect.deployments import Deployment
from prefect.server.schemas.schedules import CronSchedule

deployment = Deployment.build_from_flow(
    flow=etl_flow,
    name="etl-production",
    schedule=CronSchedule(cron="0 2 * * *"),
    work_queue_name="production",
    parameters={"source": "database"},
    tags=["production", "etl"],
)

deployment.apply()
```

### Blocks é…ç½®

```python
from prefect.blocks.system import Secret, JSON

# å­˜å‚¨å¯†é’¥
secret = Secret(value="my-secret-key")
secret.save("api-key")

# å­˜å‚¨é…ç½®
config = JSON(value={"host": "localhost", "port": 5432})
config.save("db-config")

# ä½¿ç”¨ Block
@task
def connect_db():
    """è¿æ¥æ•°æ®åº“"""
    config = JSON.load("db-config")
    api_key = Secret.load("api-key")
    print(f"Connecting to {config.value['host']}")
```

## è°ƒåº¦ç­–ç•¥

### Cron è¡¨è¾¾å¼

| è¡¨è¾¾å¼ | è¯´æ˜ | ç¤ºä¾‹ |
|--------|------|------|
| `0 2 * * *` | æ¯å¤©å‡Œæ™¨2ç‚¹ | æ—¥æ‰¹å¤„ç† |
| `0 */4 * * *` | æ¯4å°æ—¶ | å¢é‡åŒæ­¥ |
| `0 0 * * 0` | æ¯å‘¨æ—¥åˆå¤œ | å‘¨æŠ¥ç”Ÿæˆ |
| `0 0 1 * *` | æ¯æœˆ1å· | æœˆåº¦æ±‡æ€» |
| `*/15 * * * *` | æ¯15åˆ†é’Ÿ | å®æ—¶ç›‘æ§ |

### äº‹ä»¶é©±åŠ¨è°ƒåº¦

```python
# Airflow æ–‡ä»¶è§¦å‘
from airflow.sensors.filesystem import FileSensor

wait_for_file = FileSensor(
    task_id='wait_for_file',
    filepath='/data/trigger.flag',
    poke_interval=10,
)

# Dagster ä¼ æ„Ÿå™¨è§¦å‘
from dagster import sensor, RunRequest

@sensor(job=my_job)
def s3_sensor(context):
    """S3 æ–‡ä»¶åˆ°è¾¾è§¦å‘"""
    new_files = check_s3_bucket()
    for file in new_files:
        yield RunRequest(run_key=file)

# Prefect è‡ªåŠ¨åŒ–è§¦å‘
from prefect.events import DeploymentEventTrigger

trigger = DeploymentEventTrigger(
    expect={"resource.id": "s3://bucket/data"},
    match_related={"resource.type": "file"},
)
```

### ä¾èµ–è°ƒåº¦

```python
# Airflow è·¨ DAG ä¾èµ–
from airflow.sensors.external_task import ExternalTaskSensor

wait_upstream = ExternalTaskSensor(
    task_id='wait_upstream',
    external_dag_id='upstream_dag',
    external_task_id='final_task',
)

# Dagster èµ„äº§ä¾èµ–
@asset(deps=[upstream_asset])
def downstream_asset():
    pass

# Prefect å­æµç¨‹
@flow
def parent_flow():
    child_flow()
```

## é”™è¯¯å¤„ç†

### é‡è¯•ç­–ç•¥

```python
# Airflow é‡è¯•
default_args = {
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(hours=1),
}

# Dagster é‡è¯•
from dagster import RetryPolicy

@asset(
    retry_policy=RetryPolicy(
        max_retries=3,
        delay=60,
    )
)
def my_asset():
    pass

# Prefect é‡è¯•
@task(
    retries=3,
    retry_delay_seconds=60,
    retry_jitter_factor=0.5,
)
def my_task():
    pass
```

### å¤±è´¥å›è°ƒ

```python
# Airflow å›è°ƒ
def on_failure_callback(context):
    """å¤±è´¥å›è°ƒ"""
    task = context['task_instance']
    send_alert(f"Task {task.task_id} failed")

task = PythonOperator(
    task_id='my_task',
    python_callable=my_func,
    on_failure_callback=on_failure_callback,
)

# Dagster é’©å­
from dagster import failure_hook

@failure_hook
def slack_on_failure(context):
    """å¤±è´¥é€šçŸ¥"""
    send_slack_message(f"Asset {context.asset_key} failed")

@asset(hooks={slack_on_failure})
def my_asset():
    pass
```

## ç›‘æ§å‘Šè­¦

### æŒ‡æ ‡æ”¶é›†

```python
# Airflow æŒ‡æ ‡
from airflow.metrics import Stats

def my_task():
    Stats.incr('custom.task.count')
    Stats.timing('custom.task.duration', 100)
    Stats.gauge('custom.task.records', 1000)

# Dagster å…ƒæ•°æ®
from dagster import MaterializeResult

@asset
def my_asset():
    return MaterializeResult(
        metadata={
            "records_processed": 1000,
            "duration_seconds": 45.2,
        }
    )
```

### SLA ç›‘æ§

```python
# Airflow SLA
with DAG(
    'my_dag',
    default_args={
        'sla': timedelta(hours=2),
        'sla_miss_callback': sla_miss_alert,
    }
) as dag:
    task = PythonOperator(task_id='task')

# Dagster èµ„äº§æ£€æŸ¥
from dagster import asset_check, AssetCheckResult

@asset_check(asset=my_asset)
def check_freshness():
    """æ£€æŸ¥æ•°æ®æ–°é²œåº¦"""
    age = get_data_age()
    return AssetCheckResult(
        passed=age < timedelta(hours=2),
        metadata={"age_hours": age.total_seconds() / 3600}
    )
```

## æ•°æ®è¡€ç¼˜

### Airflow Lineage

```python
from airflow.lineage import AUTO
from airflow.lineage.entities import File

input_file = File("/data/input.csv")
output_file = File("/data/output.csv")

task = PythonOperator(
    task_id='transform',
    python_callable=transform_func,
    inlets={"auto": AUTO, "datasets": [input_file]},
    outlets={"datasets": [output_file]},
)
```

### Dagster è¡€ç¼˜è¿½è¸ª

```python
from dagster import AssetIn, asset

@asset
def source_data():
    """æºæ•°æ®"""
    return pd.DataFrame()

@asset(
    ins={"source": AssetIn("source_data")},
)
def transformed_data(source: pd.DataFrame):
    """è½¬æ¢æ•°æ® - è‡ªåŠ¨è¿½è¸ªè¡€ç¼˜"""
    return source.copy()
```

## æœ€ä½³å®è·µ

### å¹‚ç­‰æ€§è®¾è®¡

```python
# ä½¿ç”¨ UPSERT è€Œé INSERT
def load_data(df: pd.DataFrame):
    """å¹‚ç­‰åŠ è½½"""
    df.to_sql(
        'users',
        engine,
        if_exists='replace',  # æˆ–ä½¿ç”¨ ON CONFLICT
        index=False,
    )

# ä½¿ç”¨åˆ†åŒºè¦†ç›–
def write_partition(df: pd.DataFrame, date: str):
    """åˆ†åŒºè¦†ç›–å†™å…¥"""
    path = f"s3://bucket/data/date={date}/"
    df.to_parquet(path, mode='overwrite')
```

### å¢é‡å¤„ç†

```python
@task
def incremental_extract(last_run: datetime):
    """å¢é‡æå–"""
    query = f"""
        SELECT * FROM users
        WHERE updated_at > '{last_run}'
    """
    return pd.read_sql(query, engine)

@flow
def incremental_flow():
    """å¢é‡æµç¨‹"""
    last_run = get_last_run_time()
    new_data = incremental_extract(last_run)
    if not new_data.empty:
        transform_and_load(new_data)
```

### æ•°æ®éªŒè¯

```python
@task
def validate_data(df: pd.DataFrame):
    """æ•°æ®éªŒè¯"""
    assert not df.empty, "DataFrame is empty"
    assert df['user_id'].is_unique, "Duplicate user_id"
    assert df['email'].notna().all(), "Null emails found"
    assert df['age'].between(0, 120).all(), "Invalid age"
```

## æ¡†æ¶å¯¹æ¯”

| ç‰¹æ€§ | Airflow | Dagster | Prefect |
|------|---------|---------|---------|
| å­¦ä¹ æ›²çº¿ | é™¡å³­ | ä¸­ç­‰ | å¹³ç¼“ |
| èµ„äº§ç®¡ç† | âŒ | âœ… | âŒ |
| åŠ¨æ€ä»»åŠ¡ | âœ… | âœ… | âœ… |
| æœ¬åœ°å¼€å‘ | å¤æ‚ | ç®€å• | ç®€å• |
| UI ä½“éªŒ | ä¼ ç»Ÿ | ç°ä»£ | ç°ä»£ |
| ç¤¾åŒºç”Ÿæ€ | æœ€å¤§ | æˆé•¿ä¸­ | æˆé•¿ä¸­ |
| ä¼ä¸šæ”¯æŒ | Astronomer | Dagster+ | Prefect Cloud |

## å·¥å…·æ¸…å•

| å·¥å…· | ç”¨é€” | æ¨èåœºæ™¯ |
|------|------|----------|
| Apache Airflow | æ‰¹å¤„ç†ç¼–æ’ | å¤æ‚ DAGã€æˆç†Ÿç”Ÿæ€ |
| Dagster | èµ„äº§ç®¡ç† | æ•°æ®èµ„äº§ã€è¡€ç¼˜è¿½è¸ª |
| Prefect | ç°ä»£å·¥ä½œæµ | å¿«é€Ÿå¼€å‘ã€åŠ¨æ€æµç¨‹ |
| Luigi | è½»é‡ç¼–æ’ | ç®€å•ç®¡é“ã€Python åŸç”Ÿ |
| Argo Workflows | K8s ç¼–æ’ | äº‘åŸç”Ÿã€å®¹å™¨åŒ– |
| Temporal | æŒä¹…åŒ–å·¥ä½œæµ | é•¿æ—¶ä»»åŠ¡ã€çŠ¶æ€ç®¡ç† |

## è§¦å‘è¯

æ•°æ®ç®¡é“ã€Airflowã€Dagsterã€Prefectã€ETLã€æ•°æ®ç¼–æ’ã€DAGã€è°ƒåº¦ã€å·¥ä½œæµã€æ•°æ®è¡€ç¼˜
