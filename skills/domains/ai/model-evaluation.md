---
name: model-evaluation
description: AI æ¨¡å‹è¯„ä¼°æŠ€æœ¯ã€‚RAGASã€LLM-as-Judgeã€è¯„ä¼°æŒ‡æ ‡ã€åŸºå‡†æµ‹è¯•ã€A/B æµ‹è¯•ã€‚å½“ç”¨æˆ·æåˆ°æ¨¡å‹è¯„ä¼°ã€RAGASã€LLM-as-Judgeã€åŸºå‡†æµ‹è¯•ã€è¯„ä¼°æŒ‡æ ‡ã€æ¨¡å‹å¯¹æ¯”æ—¶ä½¿ç”¨ã€‚
---

# ğŸ“Š å¤©æœºç§˜å…¸ Â· æ¨¡å‹è¯„ä¼° (Model Evaluation)

## è¯„ä¼°ä½“ç³»

```
ç¦»çº¿è¯„ä¼° â†’ åœ¨çº¿è¯„ä¼° â†’ æŒç»­ç›‘æ§
    â”‚          â”‚           â”‚
    â”œâ”€ åŸºå‡†æµ‹è¯•  â”œâ”€ A/B æµ‹è¯•  â”œâ”€ æŒ‡æ ‡è¿½è¸ª
    â”œâ”€ äººå·¥è¯„ä¼°  â”œâ”€ ç”¨æˆ·åé¦ˆ  â”œâ”€ å¼‚å¸¸æ£€æµ‹
    â””â”€ è‡ªåŠ¨è¯„ä¼°  â””â”€ å®æ—¶åˆ†æ  â””â”€ è´¨é‡æŠ¥å‘Š
```

### è¯„ä¼°ç»´åº¦
| ç»´åº¦ | æŒ‡æ ‡ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| å‡†ç¡®æ€§ | Accuracy, F1, Precision, Recall | åˆ†ç±»ã€NER |
| ç›¸å…³æ€§ | Relevance, Context Precision | RAGã€æ£€ç´¢ |
| å¿ å®æ€§ | Faithfulness, Hallucination Rate | ç”Ÿæˆä»»åŠ¡ |
| è¿è´¯æ€§ | Coherence, Fluency | æ–‡æœ¬ç”Ÿæˆ |
| æ•ˆç‡ | Latency, Throughput, Cost | ç”Ÿäº§éƒ¨ç½² |

## RAGAS æ¡†æ¶

### æ ¸å¿ƒæŒ‡æ ‡
```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from datasets import Dataset

# å‡†å¤‡è¯„ä¼°æ•°æ®
data = {
    "question": ["ä»€ä¹ˆæ˜¯ RAGï¼Ÿ", "å¦‚ä½•ä¼˜åŒ–æ£€ç´¢ï¼Ÿ"],
    "answer": ["RAG æ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆ...", "å¯ä»¥ä½¿ç”¨æ··åˆæ£€ç´¢..."],
    "contexts": [
        ["RAG ç»“åˆäº†æ£€ç´¢å’Œç”Ÿæˆ...", "å‘é‡æ•°æ®åº“ç”¨äºå­˜å‚¨..."],
        ["æ··åˆæ£€ç´¢ç»“åˆå‘é‡å’Œå…³é”®è¯...", "é‡æ’å¯ä»¥æå‡ç›¸å…³æ€§..."]
    ],
    "ground_truth": ["RAG æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯", "ä½¿ç”¨æ··åˆæ£€ç´¢å’Œé‡æ’"]
}

dataset = Dataset.from_dict(data)

# è¯„ä¼°
result = evaluate(
    dataset,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
    ],
)

print(result)
```

### Faithfulnessï¼ˆå¿ å®æ€§ï¼‰
```python
from ragas.metrics import faithfulness

# è¡¡é‡ç­”æ¡ˆæ˜¯å¦åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡
# åˆ†æ•° 0-1ï¼Œè¶Šé«˜è¶Šå¥½

# ç¤ºä¾‹
question = "Python çš„åˆ›å§‹äººæ˜¯è°ï¼Ÿ"
context = ["Python ç”± Guido van Rossum åœ¨ 1991 å¹´åˆ›å»º"]
answer = "Python ç”± Guido van Rossum åˆ›å»º"  # é«˜å¿ å®æ€§

# ä½å¿ å®æ€§ç¤ºä¾‹
bad_answer = "Python ç”± James Gosling åˆ›å»º"  # ç¼–é€ ä¿¡æ¯

score = faithfulness.score(
    question=question,
    answer=answer,
    contexts=context
)
```

### Answer Relevancyï¼ˆç­”æ¡ˆç›¸å…³æ€§ï¼‰
```python
from ragas.metrics import answer_relevancy

# è¡¡é‡ç­”æ¡ˆä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦
# åˆ†æ•° 0-1ï¼Œè¶Šé«˜è¶Šå¥½

question = "å¦‚ä½•é˜²å¾¡ SQL æ³¨å…¥ï¼Ÿ"
answer = "ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢å’Œ ORM æ¡†æ¶å¯ä»¥æœ‰æ•ˆé˜²å¾¡ SQL æ³¨å…¥"  # é«˜ç›¸å…³æ€§
bad_answer = "SQL æ˜¯ä¸€ç§æ•°æ®åº“æŸ¥è¯¢è¯­è¨€"  # ä½ç›¸å…³æ€§

score = answer_relevancy.score(
    question=question,
    answer=answer
)
```

### Context Precisionï¼ˆä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ï¼‰
```python
from ragas.metrics import context_precision

# è¡¡é‡æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸­ç›¸å…³ä¿¡æ¯çš„æ¯”ä¾‹
# åˆ†æ•° 0-1ï¼Œè¶Šé«˜è¶Šå¥½ï¼ˆç›¸å…³æ–‡æ¡£æ’åœ¨å‰é¢ï¼‰

question = "ä»€ä¹ˆæ˜¯å‘é‡æ•°æ®åº“ï¼Ÿ"
contexts = [
    "å‘é‡æ•°æ®åº“ç”¨äºå­˜å‚¨å’Œæ£€ç´¢é«˜ç»´å‘é‡",  # ç›¸å…³
    "Pinecone æ˜¯ä¸€ä¸ªå‘é‡æ•°æ®åº“",  # ç›¸å…³
    "Python æ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€",  # ä¸ç›¸å…³
]
ground_truth = "å‘é‡æ•°æ®åº“æ˜¯ä¸“é—¨ç”¨äºå­˜å‚¨å’Œæ£€ç´¢å‘é‡çš„æ•°æ®åº“"

score = context_precision.score(
    question=question,
    contexts=contexts,
    ground_truth=ground_truth
)
```

### Context Recallï¼ˆä¸Šä¸‹æ–‡å¬å›ç‡ï¼‰
```python
from ragas.metrics import context_recall

# è¡¡é‡æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ˜¯å¦åŒ…å«å›ç­”é—®é¢˜æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯
# åˆ†æ•° 0-1ï¼Œè¶Šé«˜è¶Šå¥½

question = "RAG çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"
contexts = [
    "RAG å¯ä»¥å‡å°‘å¹»è§‰",
    "RAG å¯ä»¥ä½¿ç”¨æœ€æ–°ä¿¡æ¯"
]
ground_truth = "RAG çš„ä¼˜åŠ¿åŒ…æ‹¬å‡å°‘å¹»è§‰ã€ä½¿ç”¨æœ€æ–°ä¿¡æ¯ã€å¯è§£é‡Šæ€§å¼º"

# å¬å›ç‡ = 2/3 = 0.67ï¼ˆç¼ºå°‘"å¯è§£é‡Šæ€§å¼º"ï¼‰
```

### å®Œæ•´ RAG è¯„ä¼°æµç¨‹
```python
from langchain.chains import RetrievalQA
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy

class RAGEvaluator:
    def __init__(self, qa_chain, test_dataset):
        self.qa_chain = qa_chain
        self.test_dataset = test_dataset

    def run_evaluation(self):
        results = []

        for item in self.test_dataset:
            # æ‰§è¡Œ RAG
            response = self.qa_chain({
                "query": item["question"]
            })

            results.append({
                "question": item["question"],
                "answer": response["result"],
                "contexts": [doc.page_content for doc in response["source_documents"]],
                "ground_truth": item["ground_truth"]
            })

        # RAGAS è¯„ä¼°
        dataset = Dataset.from_dict({
            "question": [r["question"] for r in results],
            "answer": [r["answer"] for r in results],
            "contexts": [r["contexts"] for r in results],
            "ground_truth": [r["ground_truth"] for r in results]
        })

        scores = evaluate(
            dataset,
            metrics=[faithfulness, answer_relevancy]
        )

        return scores

# ä½¿ç”¨
evaluator = RAGEvaluator(qa_chain, test_data)
scores = evaluator.run_evaluation()
print(f"Faithfulness: {scores['faithfulness']:.3f}")
print(f"Answer Relevancy: {scores['answer_relevancy']:.3f}")
```

## LLM-as-Judge

### åŸºç¡€è¯„ä¼°å™¨
```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

class LLMJudge:
    def __init__(self, model="gpt-4"):
        self.llm = ChatOpenAI(model=model, temperature=0)

    def evaluate_answer(self, question: str, answer: str, criteria: str):
        prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ä»¥ä¸‹ç­”æ¡ˆçš„è´¨é‡ã€‚

é—®é¢˜: {question}

ç­”æ¡ˆ: {answer}

è¯„ä¼°æ ‡å‡†: {criteria}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„åˆ†ï¼ˆ1-5 åˆ†ï¼‰:
1. å‡†ç¡®æ€§: ä¿¡æ¯æ˜¯å¦æ­£ç¡®
2. å®Œæ•´æ€§: æ˜¯å¦å……åˆ†å›ç­”é—®é¢˜
3. æ¸…æ™°åº¦: è¡¨è¾¾æ˜¯å¦æ¸…æ™°æ˜“æ‡‚
4. ç›¸å…³æ€§: æ˜¯å¦åˆ‡é¢˜

è¾“å‡ºæ ¼å¼:
{{
  "accuracy": <åˆ†æ•°>,
  "completeness": <åˆ†æ•°>,
  "clarity": <åˆ†æ•°>,
  "relevance": <åˆ†æ•°>,
  "overall": <æ€»åˆ†>,
  "feedback": "<è¯¦ç»†åé¦ˆ>"
}}
""")

        chain = prompt | self.llm
        result = chain.invoke({
            "question": question,
            "answer": answer,
            "criteria": criteria
        })

        return json.loads(result.content)

# ä½¿ç”¨
judge = LLMJudge()
score = judge.evaluate_answer(
    question="ä»€ä¹ˆæ˜¯ RAGï¼Ÿ",
    answer="RAG æ˜¯æ£€ç´¢å¢å¼ºç”ŸæˆæŠ€æœ¯...",
    criteria="æŠ€æœ¯å‡†ç¡®æ€§å’Œæ¸…æ™°åº¦"
)
```

### æˆå¯¹æ¯”è¾ƒ
```python
def pairwise_comparison(question: str, answer_a: str, answer_b: str):
    prompt = f"""
é—®é¢˜: {question}

ç­”æ¡ˆ A: {answer_a}

ç­”æ¡ˆ B: {answer_b}

è¯·æ¯”è¾ƒä¸¤ä¸ªç­”æ¡ˆçš„è´¨é‡ï¼Œä»ä»¥ä¸‹ç»´åº¦è¯„ä¼°:
1. å‡†ç¡®æ€§
2. å®Œæ•´æ€§
3. æ¸…æ™°åº¦

é€‰æ‹©æ›´å¥½çš„ç­”æ¡ˆï¼ˆA æˆ– Bï¼‰ï¼Œå¹¶è¯´æ˜ç†ç”±ã€‚

è¾“å‡ºæ ¼å¼:
{{
  "winner": "A" or "B",
  "reason": "<ç†ç”±>",
  "confidence": <0-1>
}}
"""

    result = llm.predict(prompt)
    return json.loads(result)

# ELO æ’åç³»ç»Ÿ
class ELORanking:
    def __init__(self, k=32):
        self.k = k
        self.ratings = {}

    def update_ratings(self, model_a: str, model_b: str, winner: str):
        ra = self.ratings.get(model_a, 1500)
        rb = self.ratings.get(model_b, 1500)

        ea = 1 / (1 + 10 ** ((rb - ra) / 400))
        eb = 1 / (1 + 10 ** ((ra - rb) / 400))

        if winner == model_a:
            sa, sb = 1, 0
        elif winner == model_b:
            sa, sb = 0, 1
        else:
            sa, sb = 0.5, 0.5

        self.ratings[model_a] = ra + self.k * (sa - ea)
        self.ratings[model_b] = rb + self.k * (sb - eb)

    def get_leaderboard(self):
        return sorted(self.ratings.items(), key=lambda x: x[1], reverse=True)
```

### å¤šç»´åº¦è¯„ä¼°
```python
EVALUATION_DIMENSIONS = {
    "correctness": "ç­”æ¡ˆæ˜¯å¦å‡†ç¡®æ— è¯¯",
    "completeness": "æ˜¯å¦å®Œæ•´å›ç­”äº†é—®é¢˜",
    "conciseness": "è¡¨è¾¾æ˜¯å¦ç®€æ´",
    "relevance": "æ˜¯å¦åˆ‡é¢˜",
    "helpfulness": "å¯¹ç”¨æˆ·æ˜¯å¦æœ‰å¸®åŠ©",
    "safety": "æ˜¯å¦å®‰å…¨æ— å®³",
}

def multi_dimensional_eval(question: str, answer: str):
    results = {}

    for dimension, description in EVALUATION_DIMENSIONS.items():
        prompt = f"""
è¯„ä¼°ç»´åº¦: {dimension} - {description}

é—®é¢˜: {question}
ç­”æ¡ˆ: {answer}

è¯·å¯¹è¯¥ç»´åº¦è¯„åˆ†ï¼ˆ1-5 åˆ†ï¼‰å¹¶è¯´æ˜ç†ç”±ã€‚

è¾“å‡ºæ ¼å¼:
{{
  "score": <åˆ†æ•°>,
  "reason": "<ç†ç”±>"
}}
"""
        result = llm.predict(prompt)
        results[dimension] = json.loads(result)

    # è®¡ç®—åŠ æƒæ€»åˆ†
    weights = {
        "correctness": 0.3,
        "completeness": 0.2,
        "conciseness": 0.1,
        "relevance": 0.2,
        "helpfulness": 0.15,
        "safety": 0.05,
    }

    total_score = sum(
        results[dim]["score"] * weights[dim]
        for dim in EVALUATION_DIMENSIONS
    )

    results["total_score"] = total_score
    return results
```

## åŸºå‡†æµ‹è¯•

### MMLUï¼ˆå¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£ï¼‰
```python
from datasets import load_dataset

# åŠ è½½ MMLU æ•°æ®é›†
dataset = load_dataset("cais/mmlu", "all")

def evaluate_mmlu(model, subject="all", num_samples=100):
    correct = 0
    total = 0

    for item in dataset["test"].select(range(num_samples)):
        question = item["question"]
        choices = item["choices"]
        correct_answer = item["answer"]

        # æ„å»º Prompt
        prompt = f"""
é—®é¢˜: {question}

é€‰é¡¹:
A. {choices[0]}
B. {choices[1]}
C. {choices[2]}
D. {choices[3]}

è¯·é€‰æ‹©æ­£ç¡®ç­”æ¡ˆï¼ˆä»…è¾“å‡º A/B/C/Dï¼‰:
"""

        response = model.predict(prompt).strip()

        if response == ["A", "B", "C", "D"][correct_answer]:
            correct += 1
        total += 1

    accuracy = correct / total
    return accuracy

# ä½¿ç”¨
accuracy = evaluate_mmlu(llm, num_samples=100)
print(f"MMLU Accuracy: {accuracy:.2%}")
```

### HumanEvalï¼ˆä»£ç ç”Ÿæˆï¼‰
```python
from human_eval.data import read_problems
from human_eval.evaluation import evaluate_functional_correctness

def evaluate_code_generation(model):
    problems = read_problems()

    samples = []
    for task_id, problem in problems.items():
        prompt = problem["prompt"]

        # ç”Ÿæˆä»£ç 
        code = model.predict(f"å®Œæˆä»¥ä¸‹ Python å‡½æ•°:\n\n{prompt}")

        samples.append({
            "task_id": task_id,
            "completion": code
        })

    # ä¿å­˜ç»“æœ
    with open("samples.jsonl", "w") as f:
        for sample in samples:
            f.write(json.dumps(sample) + "\n")

    # è¯„ä¼°
    results = evaluate_functional_correctness("samples.jsonl")
    return results

# Pass@k æŒ‡æ ‡
# Pass@1: ç”Ÿæˆ 1 æ¬¡ä»£ç çš„é€šè¿‡ç‡
# Pass@10: ç”Ÿæˆ 10 æ¬¡ä»£ç ä¸­è‡³å°‘ 1 æ¬¡é€šè¿‡çš„æ¦‚ç‡
```

### GSM8Kï¼ˆæ•°å­¦æ¨ç†ï¼‰
```python
from datasets import load_dataset

dataset = load_dataset("gsm8k", "main")

def evaluate_gsm8k(model, num_samples=100):
    correct = 0

    for item in dataset["test"].select(range(num_samples)):
        question = item["question"]
        answer = item["answer"]

        # æå–æ­£ç¡®ç­”æ¡ˆ
        correct_answer = int(answer.split("####")[1].strip())

        # ä½¿ç”¨ CoT
        prompt = f"""
é—®é¢˜: {question}

è®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ€è€ƒ:
"""

        response = model.predict(prompt)

        # æå–æ¨¡å‹ç­”æ¡ˆ
        try:
            model_answer = extract_number(response)
            if model_answer == correct_answer:
                correct += 1
        except:
            pass

    accuracy = correct / num_samples
    return accuracy
```

### è‡ªå®šä¹‰åŸºå‡†æµ‹è¯•
```python
class CustomBenchmark:
    def __init__(self, test_cases: list):
        self.test_cases = test_cases

    def run(self, model):
        results = []

        for case in self.test_cases:
            start_time = time.time()

            response = model.predict(case["input"])

            latency = time.time() - start_time

            # è¯„ä¼°
            score = self._evaluate(
                response,
                case["expected_output"],
                case["criteria"]
            )

            results.append({
                "input": case["input"],
                "output": response,
                "expected": case["expected_output"],
                "score": score,
                "latency": latency
            })

        return self._aggregate_results(results)

    def _evaluate(self, output, expected, criteria):
        # ä½¿ç”¨ LLM-as-Judge
        judge = LLMJudge()
        return judge.evaluate(output, expected, criteria)

    def _aggregate_results(self, results):
        return {
            "avg_score": np.mean([r["score"] for r in results]),
            "avg_latency": np.mean([r["latency"] for r in results]),
            "pass_rate": sum(r["score"] >= 0.8 for r in results) / len(results),
            "details": results
        }
```

## è¯„ä¼°æŒ‡æ ‡

### åˆ†ç±»æŒ‡æ ‡
```python
from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support,
    confusion_matrix,
    classification_report
)

def evaluate_classification(y_true, y_pred):
    accuracy = accuracy_score(y_true, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average='weighted'
    )

    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "confusion_matrix": confusion_matrix(y_true, y_pred),
        "report": classification_report(y_true, y_pred)
    }
```

### ç”ŸæˆæŒ‡æ ‡
```python
from rouge import Rouge
from nltk.translate.bleu_score import sentence_bleu

def evaluate_generation(predictions, references):
    rouge = Rouge()

    # ROUGE åˆ†æ•°
    rouge_scores = rouge.get_scores(predictions, references, avg=True)

    # BLEU åˆ†æ•°
    bleu_scores = []
    for pred, ref in zip(predictions, references):
        score = sentence_bleu([ref.split()], pred.split())
        bleu_scores.append(score)

    return {
        "rouge-1": rouge_scores["rouge-1"]["f"],
        "rouge-2": rouge_scores["rouge-2"]["f"],
        "rouge-l": rouge_scores["rouge-l"]["f"],
        "bleu": np.mean(bleu_scores)
    }
```

### æ£€ç´¢æŒ‡æ ‡
```python
def evaluate_retrieval(retrieved_docs, relevant_docs, k=5):
    # Precision@K
    precision_at_k = len(set(retrieved_docs[:k]) & set(relevant_docs)) / k

    # Recall@K
    recall_at_k = len(set(retrieved_docs[:k]) & set(relevant_docs)) / len(relevant_docs)

    # MRR (Mean Reciprocal Rank)
    for i, doc in enumerate(retrieved_docs, 1):
        if doc in relevant_docs:
            mrr = 1 / i
            break
    else:
        mrr = 0

    # NDCG (Normalized Discounted Cumulative Gain)
    dcg = sum(
        (1 if retrieved_docs[i] in relevant_docs else 0) / np.log2(i + 2)
        for i in range(k)
    )
    idcg = sum(1 / np.log2(i + 2) for i in range(min(k, len(relevant_docs))))
    ndcg = dcg / idcg if idcg > 0 else 0

    return {
        "precision@k": precision_at_k,
        "recall@k": recall_at_k,
        "mrr": mrr,
        "ndcg@k": ndcg
    }
```

## A/B æµ‹è¯•

### åœ¨çº¿ A/B æµ‹è¯•æ¡†æ¶
```python
import random
from dataclasses import dataclass
from typing import Dict

@dataclass
class Variant:
    name: str
    model: Any
    traffic_ratio: float

class ABTest:
    def __init__(self, variants: list[Variant]):
        self.variants = variants
        self.results = {v.name: {"count": 0, "scores": []} for v in variants}

    def get_variant(self, user_id: str) -> Variant:
        # ä¸€è‡´æ€§å“ˆå¸Œåˆ†æµ
        hash_value = hash(user_id) % 100
        cumulative = 0

        for variant in self.variants:
            cumulative += variant.traffic_ratio * 100
            if hash_value < cumulative:
                return variant

        return self.variants[-1]

    def log_result(self, variant_name: str, score: float):
        self.results[variant_name]["count"] += 1
        self.results[variant_name]["scores"].append(score)

    def get_statistics(self):
        stats = {}
        for name, data in self.results.items():
            if data["count"] > 0:
                stats[name] = {
                    "count": data["count"],
                    "mean": np.mean(data["scores"]),
                    "std": np.std(data["scores"]),
                    "p95": np.percentile(data["scores"], 95)
                }
        return stats

# ä½¿ç”¨
ab_test = ABTest([
    Variant("gpt-4", gpt4_model, 0.5),
    Variant("claude-3", claude_model, 0.5)
])

# å¤„ç†è¯·æ±‚
user_id = "user_123"
variant = ab_test.get_variant(user_id)
response = variant.model.predict(query)

# è®°å½•ç»“æœï¼ˆç”¨æˆ·åé¦ˆæˆ–è‡ªåŠ¨è¯„ä¼°ï¼‰
score = evaluate_response(response)
ab_test.log_result(variant.name, score)

# æŸ¥çœ‹ç»Ÿè®¡
print(ab_test.get_statistics())
```

### ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
```python
from scipy import stats

def check_significance(results_a, results_b, alpha=0.05):
    # t æ£€éªŒ
    t_stat, p_value = stats.ttest_ind(results_a, results_b)

    is_significant = p_value < alpha

    # æ•ˆåº”é‡ï¼ˆCohen's dï¼‰
    pooled_std = np.sqrt(
        (np.std(results_a) ** 2 + np.std(results_b) ** 2) / 2
    )
    cohens_d = (np.mean(results_a) - np.mean(results_b)) / pooled_std

    return {
        "p_value": p_value,
        "is_significant": is_significant,
        "cohens_d": cohens_d,
        "interpretation": "large" if abs(cohens_d) > 0.8 else "medium" if abs(cohens_d) > 0.5 else "small"
    }
```

## æŒç»­ç›‘æ§

### å®æ—¶æŒ‡æ ‡è¿½è¸ª
```python
from prometheus_client import Counter, Histogram, Gauge

# å®šä¹‰æŒ‡æ ‡
request_count = Counter('llm_requests_total', 'Total LLM requests', ['model', 'status'])
latency = Histogram('llm_latency_seconds', 'LLM latency', ['model'])
quality_score = Gauge('llm_quality_score', 'LLM quality score', ['model'])

class MonitoredLLM:
    def __init__(self, model, model_name):
        self.model = model
        self.model_name = model_name

    def predict(self, prompt: str):
        start_time = time.time()

        try:
            response = self.model.predict(prompt)
            request_count.labels(model=self.model_name, status='success').inc()

            # è®°å½•å»¶è¿Ÿ
            latency.labels(model=self.model_name).observe(time.time() - start_time)

            # è¯„ä¼°è´¨é‡
            score = self._evaluate_quality(response)
            quality_score.labels(model=self.model_name).set(score)

            return response

        except Exception as e:
            request_count.labels(model=self.model_name, status='error').inc()
            raise
```

### å¼‚å¸¸æ£€æµ‹
```python
class AnomalyDetector:
    def __init__(self, window_size=100, threshold=2.0):
        self.window_size = window_size
        self.threshold = threshold
        self.history = []

    def check(self, value: float) -> bool:
        self.history.append(value)

        if len(self.history) < self.window_size:
            return False

        # ä¿æŒçª—å£å¤§å°
        self.history = self.history[-self.window_size:]

        # Z-score å¼‚å¸¸æ£€æµ‹
        mean = np.mean(self.history)
        std = np.std(self.history)

        if std == 0:
            return False

        z_score = abs((value - mean) / std)

        return z_score > self.threshold

# ä½¿ç”¨
detector = AnomalyDetector()

for score in quality_scores:
    if detector.check(score):
        alert(f"Quality anomaly detected: {score}")
```

## è¯„ä¼°å·¥å…·

| å·¥å…· | ç±»å‹ | åŠŸèƒ½ |
|------|------|------|
| RAGAS | æ¡†æ¶ | RAG ä¸“ç”¨è¯„ä¼° |
| LangSmith | å¹³å° | LLM åº”ç”¨ç›‘æ§ |
| Phoenix | å¼€æº | å¯è§‚æµ‹æ€§å¹³å° |
| PromptTools | åº“ | Prompt æµ‹è¯• |
| OpenAI Evals | æ¡†æ¶ | æ¨¡å‹è¯„ä¼° |
| Weights & Biases | å¹³å° | å®éªŒè¿½è¸ª |

## æœ€ä½³å®è·µ

- âœ… å¤šç»´åº¦è¯„ä¼°ï¼šå‡†ç¡®æ€§ã€ç›¸å…³æ€§ã€å¿ å®æ€§ã€æ•ˆç‡
- âœ… è‡ªåŠ¨åŒ–è¯„ä¼°ï¼šä½¿ç”¨ RAGASã€LLM-as-Judge
- âœ… äººå·¥æŠ½æ£€ï¼šå®šæœŸäººå·¥å®¡æ ¸æ ·æœ¬
- âœ… åŸºå‡†æµ‹è¯•ï¼šä½¿ç”¨æ ‡å‡†æ•°æ®é›†å¯¹æ¯”
- âœ… A/B æµ‹è¯•ï¼šåœ¨çº¿å¯¹æ¯”ä¸åŒç‰ˆæœ¬
- âœ… æŒç»­ç›‘æ§ï¼šå®æ—¶è¿½è¸ªè´¨é‡æŒ‡æ ‡
- âœ… ç‰ˆæœ¬ç®¡ç†ï¼šè®°å½•æ¨¡å‹å’Œ Prompt ç‰ˆæœ¬
- âœ… åé¦ˆé—­ç¯ï¼šæ”¶é›†ç”¨æˆ·åé¦ˆæ”¹è¿›
- âŒ é¿å…ï¼šå•ä¸€æŒ‡æ ‡ã€æ— åŸºå‡†ã€æ— ç›‘æ§

---
