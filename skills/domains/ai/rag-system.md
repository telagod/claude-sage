---
name: rag-system
description: RAG æ£€ç´¢å¢å¼ºç”Ÿæˆæ¶æ„ã€‚å‘é‡æ•°æ®åº“ã€Embeddingã€æ£€ç´¢ç­–ç•¥ã€é‡æ’ç®—æ³•ã€æ··åˆæ£€ç´¢ã€‚å½“ç”¨æˆ·æåˆ° RAGã€æ£€ç´¢å¢å¼ºã€å‘é‡æ•°æ®åº“ã€Embeddingã€é‡æ’ã€LangChainã€LlamaIndex æ—¶ä½¿ç”¨ã€‚
---

# ğŸ”® ä¸¹é¼ç§˜å…¸ Â· RAG ç³»ç»Ÿ (Retrieval-Augmented Generation)

## RAG æ¶æ„

```
æŸ¥è¯¢ â†’ Embedding â†’ å‘é‡æ£€ç´¢ â†’ é‡æ’ â†’ ä¸Šä¸‹æ–‡æ³¨å…¥ â†’ LLM ç”Ÿæˆ
  â”‚         â”‚           â”‚         â”‚          â”‚            â”‚
  â””â”€ æ”¹å†™ â”€â”€â”´â”€ æ··åˆæ£€ç´¢ â”€â”´â”€ ç›¸å…³æ€§ â”€â”´â”€ å‹ç¼© â”€â”€â”´â”€ ç­”æ¡ˆ + å¼•ç”¨
```

### æ ¸å¿ƒæµç¨‹
```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

# 1. æ–‡æ¡£åŠ è½½ä¸åˆ‡åˆ†
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

loader = TextLoader("docs.txt")
documents = loader.load()

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", "ã€‚", ".", " "]
)
chunks = splitter.split_documents(documents)

# 2. å‘é‡åŒ–ä¸å­˜å‚¨
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(chunks, embeddings)

# 3. æ£€ç´¢ä¸ç”Ÿæˆ
llm = ChatOpenAI(model="gpt-4", temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    return_source_documents=True
)

result = qa_chain({"query": "ä»€ä¹ˆæ˜¯ RAGï¼Ÿ"})
print(result["result"])
```

## å‘é‡æ•°æ®åº“å¯¹æ¯”

| æ•°æ®åº“ | ç±»å‹ | ç´¢å¼•ç®—æ³• | é€‚ç”¨åœºæ™¯ | éƒ¨ç½² |
|--------|------|----------|----------|------|
| Pinecone | æ‰˜ç®¡ | HNSW | ç”Ÿäº§çº§ã€é«˜å¹¶å‘ | äº‘ç«¯ |
| Weaviate | å¼€æº | HNSW | å¤šæ¨¡æ€ã€GraphQL | è‡ªæ‰˜ç®¡/äº‘ |
| Qdrant | å¼€æº | HNSW | é«˜æ€§èƒ½ã€è¿‡æ»¤ | è‡ªæ‰˜ç®¡/äº‘ |
| Chroma | å¼€æº | HNSW | å¿«é€ŸåŸå‹ã€æœ¬åœ° | æœ¬åœ°/å†…å­˜ |
| Milvus | å¼€æº | IVF/HNSW | å¤§è§„æ¨¡ã€åˆ†å¸ƒå¼ | è‡ªæ‰˜ç®¡ |
| Faiss | åº“ | IVF/PQ | ç ”ç©¶ã€ç¦»çº¿ | æœ¬åœ° |

### Pinecone ç¤ºä¾‹
```python
import pinecone
from langchain.vectorstores import Pinecone

pinecone.init(api_key="YOUR_KEY", environment="us-west1-gcp")

index_name = "rag-index"
if index_name not in pinecone.list_indexes():
    pinecone.create_index(
        name=index_name,
        dimension=1536,  # OpenAI ada-002
        metric="cosine"
    )

vectorstore = Pinecone.from_documents(
    documents=chunks,
    embedding=embeddings,
    index_name=index_name
)
```

### Qdrant ç¤ºä¾‹
```python
from qdrant_client import QdrantClient
from langchain.vectorstores import Qdrant

client = QdrantClient(host="localhost", port=6333)

vectorstore = Qdrant.from_documents(
    documents=chunks,
    embedding=embeddings,
    collection_name="knowledge_base",
    client=client
)

# å¸¦è¿‡æ»¤çš„æ£€ç´¢
results = vectorstore.similarity_search(
    query="RAG æ¶æ„",
    k=5,
    filter={"source": "technical_docs"}
)
```

## Embedding æ¨¡å‹é€‰æ‹©

### æ¨¡å‹å¯¹æ¯”
| æ¨¡å‹ | ç»´åº¦ | æ€§èƒ½ | æˆæœ¬ | é€‚ç”¨åœºæ™¯ |
|------|------|------|------|----------|
| OpenAI ada-002 | 1536 | é«˜ | ä¸­ | é€šç”¨ã€å¤šè¯­è¨€ |
| Cohere embed-v3 | 1024 | é«˜ | ä¸­ | å¤šè¯­è¨€ã€å‹ç¼© |
| BGE-large-zh | 1024 | é«˜ | å…è´¹ | ä¸­æ–‡ä¼˜åŒ– |
| E5-large-v2 | 1024 | ä¸­ | å…è´¹ | å¼€æºã€é€šç”¨ |
| text2vec-base | 768 | ä¸­ | å…è´¹ | ä¸­æ–‡ã€è½»é‡ |

### æœ¬åœ° Embedding
```python
from langchain.embeddings import HuggingFaceEmbeddings

# BGE ä¸­æ–‡æ¨¡å‹
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-zh-v1.5",
    model_kwargs={'device': 'cuda'},
    encode_kwargs={'normalize_embeddings': True}
)

# æ‰¹é‡ç¼–ç 
texts = ["æ–‡æ¡£1", "æ–‡æ¡£2", "æ–‡æ¡£3"]
vectors = embeddings.embed_documents(texts)

# æŸ¥è¯¢ç¼–ç ï¼ˆå¸¦æŒ‡ä»¤ï¼‰
query_vector = embeddings.embed_query("ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤º")
```

### å¤šæ¨¡æ€ Embedding
```python
from langchain.embeddings import OpenAIEmbeddings

# CLIP å›¾æ–‡è”åˆ
class MultiModalEmbedding:
    def __init__(self):
        self.text_model = OpenAIEmbeddings()
        self.image_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")

    def embed_image(self, image_path: str):
        image = Image.open(image_path)
        return self.image_model.encode_image(image)

    def embed_text(self, text: str):
        return self.text_model.embed_query(text)
```

## æ£€ç´¢ç­–ç•¥

### Dense æ£€ç´¢ï¼ˆå‘é‡ï¼‰
```python
# ä½™å¼¦ç›¸ä¼¼åº¦æ£€ç´¢
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}
)

# MMRï¼ˆæœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼‰- å¤šæ ·æ€§
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 5, "fetch_k": 20, "lambda_mult": 0.5}
)

# ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"score_threshold": 0.8, "k": 5}
)
```

### Sparse æ£€ç´¢ï¼ˆBM25ï¼‰
```python
from langchain.retrievers import BM25Retriever

# BM25 å…³é”®è¯æ£€ç´¢
bm25_retriever = BM25Retriever.from_documents(chunks)
bm25_retriever.k = 5

results = bm25_retriever.get_relevant_documents("RAG ç³»ç»Ÿ")
```

### Hybrid æ··åˆæ£€ç´¢
```python
from langchain.retrievers import EnsembleRetriever

# å‘é‡ + BM25 æ··åˆ
ensemble_retriever = EnsembleRetriever(
    retrievers=[vectorstore.as_retriever(), bm25_retriever],
    weights=[0.6, 0.4]  # å‘é‡æƒé‡ 60%ï¼ŒBM25 æƒé‡ 40%
)

results = ensemble_retriever.get_relevant_documents("æŸ¥è¯¢")
```

### å¤šè·¯å¬å›
```python
class MultiRecallRetriever:
    def __init__(self, vector_store, bm25_retriever, graph_retriever):
        self.retrievers = {
            "vector": vector_store.as_retriever(search_kwargs={"k": 10}),
            "bm25": bm25_retriever,
            "graph": graph_retriever
        }

    def retrieve(self, query: str, top_k: int = 5):
        all_docs = []
        for name, retriever in self.retrievers.items():
            docs = retriever.get_relevant_documents(query)
            all_docs.extend([(doc, name) for doc in docs])

        # å»é‡ + é‡æ’
        unique_docs = self._deduplicate(all_docs)
        return self._rerank(unique_docs, query)[:top_k]
```

## é‡æ’ç®—æ³•

### Cross-Encoder é‡æ’
```python
from sentence_transformers import CrossEncoder

class Reranker:
    def __init__(self):
        self.model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

    def rerank(self, query: str, documents: list, top_k: int = 5):
        pairs = [[query, doc.page_content] for doc in documents]
        scores = self.model.predict(pairs)

        # æŒ‰åˆ†æ•°æ’åº
        ranked = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)
        return [doc for doc, score in ranked[:top_k]]

# ä½¿ç”¨
reranker = Reranker()
initial_docs = vectorstore.similarity_search(query, k=20)
final_docs = reranker.rerank(query, initial_docs, top_k=5)
```

### Cohere Rerank API
```python
import cohere

co = cohere.Client("YOUR_API_KEY")

def cohere_rerank(query: str, documents: list, top_k: int = 5):
    results = co.rerank(
        query=query,
        documents=[doc.page_content for doc in documents],
        top_n=top_k,
        model="rerank-multilingual-v2.0"
    )

    return [documents[r.index] for r in results]
```

### LLM é‡æ’
```python
from langchain.chat_models import ChatOpenAI

def llm_rerank(query: str, documents: list, top_k: int = 3):
    llm = ChatOpenAI(model="gpt-4", temperature=0)

    prompt = f"""ç»™å®šæŸ¥è¯¢å’Œæ–‡æ¡£åˆ—è¡¨ï¼ŒæŒ‰ç›¸å…³æ€§æ’åºï¼ˆ1æœ€ç›¸å…³ï¼‰ã€‚

æŸ¥è¯¢: {query}

æ–‡æ¡£:
{chr(10).join([f"{i+1}. {doc.page_content[:200]}" for i, doc in enumerate(documents)])}

è¾“å‡ºæ ¼å¼: 1,3,2,5,4ï¼ˆä»…æ•°å­—å’Œé€—å·ï¼‰"""

    ranking = llm.predict(prompt).strip().split(',')
    return [documents[int(i)-1] for i in ranking[:top_k]]
```

## æ–‡æ¡£åˆ‡åˆ†ç­–ç•¥

### é€’å½’åˆ‡åˆ†
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    separators=["\n\n", "\n", "ã€‚", ".", " ", ""]
)
```

### è¯­ä¹‰åˆ‡åˆ†
```python
from langchain.text_splitter import SemanticChunker

semantic_splitter = SemanticChunker(
    embeddings=embeddings,
    breakpoint_threshold_type="percentile",  # æˆ– "standard_deviation"
    breakpoint_threshold_amount=95
)

chunks = semantic_splitter.split_text(long_text)
```

### Markdown ç»“æ„åŒ–åˆ‡åˆ†
```python
from langchain.text_splitter import MarkdownHeaderTextSplitter

headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
]

markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)
chunks = markdown_splitter.split_text(markdown_text)
```

## æŸ¥è¯¢ä¼˜åŒ–

### æŸ¥è¯¢æ”¹å†™
```python
from langchain.prompts import ChatPromptTemplate

query_rewrite_prompt = ChatPromptTemplate.from_template("""
å°†ç”¨æˆ·æŸ¥è¯¢æ”¹å†™ä¸ºæ›´é€‚åˆæ£€ç´¢çš„å½¢å¼ã€‚

åŸå§‹æŸ¥è¯¢: {query}

æ”¹å†™è¦æ±‚:
1. è¡¥å…¨çœç•¥ä¿¡æ¯
2. æ‰©å±•åŒä¹‰è¯
3. æ‹†åˆ†å¤åˆé—®é¢˜

æ”¹å†™åæŸ¥è¯¢:""")

def rewrite_query(query: str):
    chain = query_rewrite_prompt | llm
    return chain.invoke({"query": query}).content
```

### å¤šæŸ¥è¯¢ç”Ÿæˆ
```python
from langchain.retrievers.multi_query import MultiQueryRetriever

multi_query_retriever = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(),
    llm=llm
)

# è‡ªåŠ¨ç”Ÿæˆ 3-5 ä¸ªå˜ä½“æŸ¥è¯¢
results = multi_query_retriever.get_relevant_documents("RAG æ˜¯ä»€ä¹ˆï¼Ÿ")
```

### HyDEï¼ˆå‡è®¾æ–‡æ¡£åµŒå…¥ï¼‰
```python
def hyde_retrieval(query: str):
    # 1. è®© LLM ç”Ÿæˆå‡è®¾ç­”æ¡ˆ
    hyde_prompt = f"è¯·è¯¦ç»†å›ç­”: {query}"
    hypothetical_doc = llm.predict(hyde_prompt)

    # 2. ç”¨å‡è®¾ç­”æ¡ˆæ£€ç´¢
    results = vectorstore.similarity_search(hypothetical_doc, k=5)
    return results
```

## ä¸Šä¸‹æ–‡å‹ç¼©

### LLM å‹ç¼©å™¨
```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

compressor = LLMChainExtractor.from_llm(llm)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectorstore.as_retriever(search_kwargs={"k": 10})
)

# æ£€ç´¢ 10 ä¸ªæ–‡æ¡£ï¼Œå‹ç¼©åè¿”å›æœ€ç›¸å…³ç‰‡æ®µ
compressed_docs = compression_retriever.get_relevant_documents(query)
```

### Embedding è¿‡æ»¤
```python
from langchain.retrievers.document_compressors import EmbeddingsFilter

embeddings_filter = EmbeddingsFilter(
    embeddings=embeddings,
    similarity_threshold=0.76
)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=embeddings_filter,
    base_retriever=vectorstore.as_retriever(search_kwargs={"k": 20})
)
```

## å®Œæ•´ RAG Pipeline

### LangChain å®ç°
```python
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# è®°å¿†
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"
)

# å¯¹è¯å¼ RAG
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    memory=memory,
    return_source_documents=True,
    verbose=True
)

# å¤šè½®å¯¹è¯
result1 = qa_chain({"question": "ä»€ä¹ˆæ˜¯ RAGï¼Ÿ"})
result2 = qa_chain({"question": "å®ƒæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ"})  # è‡ªåŠ¨å¼•ç”¨ä¸Šä¸‹æ–‡
```

### LlamaIndex å®ç°
```python
from llama_index import VectorStoreIndex, ServiceContext
from llama_index.llms import OpenAI
from llama_index.embeddings import OpenAIEmbedding

# æœåŠ¡ä¸Šä¸‹æ–‡
service_context = ServiceContext.from_defaults(
    llm=OpenAI(model="gpt-4", temperature=0),
    embed_model=OpenAIEmbedding()
)

# æ„å»ºç´¢å¼•
index = VectorStoreIndex.from_documents(
    documents,
    service_context=service_context
)

# æŸ¥è¯¢å¼•æ“
query_engine = index.as_query_engine(
    similarity_top_k=5,
    response_mode="compact"  # æˆ– "tree_summarize", "refine"
)

response = query_engine.query("ä»€ä¹ˆæ˜¯ RAGï¼Ÿ")
print(response.response)
print(response.source_nodes)  # å¼•ç”¨æ¥æº
```

## é«˜çº§ RAG æ¨¡å¼

### Self-RAGï¼ˆè‡ªæˆ‘åæ€ï¼‰
```python
class SelfRAG:
    def __init__(self, llm, retriever):
        self.llm = llm
        self.retriever = retriever

    def query(self, question: str):
        # 1. åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢
        need_retrieval = self._check_retrieval_need(question)

        if not need_retrieval:
            return self.llm.predict(question)

        # 2. æ£€ç´¢
        docs = self.retriever.get_relevant_documents(question)

        # 3. ç”Ÿæˆç­”æ¡ˆ
        answer = self._generate_with_docs(question, docs)

        # 4. è‡ªæˆ‘è¯„ä¼°
        if self._verify_answer(question, answer, docs):
            return answer
        else:
            # é‡æ–°æ£€ç´¢æˆ–ç”Ÿæˆ
            return self._fallback_generate(question)
```

### RAPTORï¼ˆé€’å½’æ‘˜è¦ï¼‰
```python
from langchain.chains.summarize import load_summarize_chain

def raptor_indexing(documents, levels=3):
    current_docs = documents
    all_summaries = []

    for level in range(levels):
        # èšç±»
        clusters = cluster_documents(current_docs, n_clusters=10)

        # æ¯ä¸ªç°‡ç”Ÿæˆæ‘˜è¦
        summaries = []
        for cluster in clusters:
            summary = summarize_chain.run(cluster)
            summaries.append(summary)

        all_summaries.extend(summaries)
        current_docs = summaries

    # ç´¢å¼•åŸæ–‡æ¡£ + å„å±‚æ‘˜è¦
    vectorstore.add_documents(documents + all_summaries)
```

## å·¥å…·ä¸æ¡†æ¶

| å·¥å…· | ç±»å‹ | ç‰¹ç‚¹ |
|------|------|------|
| LangChain | æ¡†æ¶ | ç”Ÿæ€ä¸°å¯Œã€ç»„ä»¶åŒ– |
| LlamaIndex | æ¡†æ¶ | ç´¢å¼•ä¼˜åŒ–ã€æŸ¥è¯¢å¼•æ“ |
| Haystack | æ¡†æ¶ | ç”Ÿäº§çº§ã€Pipeline |
| Pinecone | å‘é‡åº“ | æ‰˜ç®¡ã€é«˜æ€§èƒ½ |
| Qdrant | å‘é‡åº“ | å¼€æºã€è¿‡æ»¤å¼º |
| Weaviate | å‘é‡åº“ | å¤šæ¨¡æ€ã€GraphQL |
| Cohere | API | Embedding + Rerank |

## æœ€ä½³å®è·µ

- âœ… æ–‡æ¡£åˆ‡åˆ†ï¼šchunk_size 500-1500ï¼Œoverlap 10-20%
- âœ… æ£€ç´¢æ•°é‡ï¼šåˆå¬å› 10-20ï¼Œé‡æ’å 3-5
- âœ… æ··åˆæ£€ç´¢ï¼šå‘é‡ + BM25 æƒé‡ 6:4 æˆ– 7:3
- âœ… å…ƒæ•°æ®è¿‡æ»¤ï¼šæ—¶é—´ã€æ¥æºã€ç±»å‹
- âœ… å¼•ç”¨æ¥æºï¼šè¿”å› source_documents
- âœ… ç¼“å­˜ï¼šç›¸åŒæŸ¥è¯¢ç¼“å­˜ç»“æœ
- âœ… ç›‘æ§ï¼šæ£€ç´¢å»¶è¿Ÿã€ç›¸å…³æ€§ã€ç­”æ¡ˆè´¨é‡
- âŒ é¿å…ï¼šchunk è¿‡å¤§/è¿‡å°ã€æ— é‡æ’ã€æ— å‹ç¼©

---
